# Skill: CVE Fix
# Automated CVE remediation for Python dependencies
# Queries Jira for CVEs, updates Pipfile and Pipfile.lock, creates MRs
#
# LESSONS LEARNED (from production use):
#
# 1. DEPENDENCY CASCADE: Upgrading one package often breaks others:
#    - aiohttp 3.13+ broke gql -> required backoff + requests-toolbelt
#    - urllib3 2.x broke boto3/botocore -> required upgrading both to 1.34.46+
#    The skill should research compatibility BEFORE updating.
#
# 2. BOTH FILES MUST UPDATE: Always update Pipfile AND Pipfile.lock together.
#    Missing Pipfile changes caused CI failures.
#
# 3. BRANCH ISOLATION: Always create branches from origin/main, never from
#    another feature branch. Cross-contamination caused multiple MR issues.
#
# 4. ERROR HANDLING: Use isinstance() checks before .get() on variables that
#    might be error strings instead of dicts (e.g., when pipenv lock fails).
#
# 5. LOCAL VALIDATION: Build container + run pytest --collect-only to catch
#    import errors from missing transitive dependencies before CI.
#
# 6. REBASE CONFLICTS: When multiple CVE fixes are in flight, document which
#    packages were modified to help resolve conflicts.

name: cve_fix
description: |
  Automatically fix CVE vulnerabilities in Python dependencies.

  Workflow:
  1. Query Jira for CVEs with downstream component "automation-analytics-backend"
  2. Filter to CVEs not already fixed (no commit mentioning CVE-XXXX in git log)
  3. Assign to current user and set to In Progress
  4. Create feature branch FROM ORIGIN/MAIN (critical for branch isolation)
  5. Research package compatibility requirements (check for known breaking changes)
  6. Read Python version from original Pipfile's [requires] section
  7. Create temp pipenv in /tmp with that Python version
  8. Install the vulnerable package (latest version) into temp pipenv
  9. Check for transitive dependency changes that may break existing code
  10. Extract new version and hashes from temp Pipfile.lock
  11. Update original Pipfile: add/update package with >= version constraint AND CVE comment
  12. Update original Pipfile.lock: update version and hashes
  13. If companion packages need upgrading (e.g., boto3 for urllib3), update those too
  14. Commit both Pipfile and Pipfile.lock changes
  15. Create MR with CVE details including compatibility notes
  16. Add GitLab MR link to Jira issue

  Known Compatibility Issues (add more as discovered):
  - urllib3 2.x requires boto3/botocore >= 1.34.46
  - aiohttp 3.13+ requires gql >= 3.5.0 (which requires backoff, requests-toolbelt)

  Uses MCP tools: jira_search, jira_assign, jira_transition, jira_add_comment,
                  git_branch_create, git_commit, git_push, gitlab_mr_create

version: "1.1"

links:
  depends_on: []            # Standalone - creates branches from main
  validates: []             # CVE fix is a specialized workflow
  validated_by:
    - check_ci_health       # Pipeline validates CVE fix builds
    - review_pr             # Code review validates the fix
    - test_mr_ephemeral     # Deployment validates fix doesn't break app
    - scan_vulnerabilities   # Vuln scan validates CVE is actually fixed
  chains_to:
    - create_mr             # Create MR with the CVE fix
    - review_pr             # Get review on the CVE fix
    - test_mr_ephemeral     # Test fix in ephemeral
    - notify_team           # Announce CVE fix
    - scan_vulnerabilities  # Verify CVE is resolved after fix
  provides_context_for:
    - scan_vulnerabilities   # CVE fix informs next vulnerability scan
    - weekly_summary        # CVE fixes in weekly report

inputs:
  - name: downstream_component
    type: string
    required: false
    default: "automation-analytics-backend"
    description: "Downstream component name to filter CVEs"

  - name: repo
    type: string
    required: false
    default: ""
    description: "Repository path - if not provided, resolved from config"

  - name: repo_name
    type: string
    required: false
    default: "automation-analytics-backend"
    description: "Repository name from config"

  - name: dry_run
    type: boolean
    required: false
    default: false
    description: "Show what would be done without making changes"

  - name: max_cves
    type: integer
    required: false
    default: 1
    description: "Maximum number of CVEs to process (default: 1 at a time)"

steps:
  # ==================== LOAD DEVELOPER PERSONA ====================

  - name: load_developer_persona
    description: "Load developer persona for Git, GitLab, and Jira tools"
    tool: persona_load
    args:
      persona_name: "developer"

  # ==================== RESOLVE REPOSITORY ====================

  - name: resolve_repo
    description: "Determine which repo to use"
    compute: |
      from scripts.common.repo_utils import resolve_repo

      resolved = resolve_repo(
          repo_path=inputs.get("repo") if inputs.get("repo") and inputs.get("repo") not in ("", ".") else None,
          repo_name=inputs.get("repo_name") if inputs.get("repo_name") else None,
      )

      result = {
          "path": resolved.path,
          "gitlab_project": resolved.gitlab_project or "automation-analytics/automation-analytics-backend",
          "default_branch": resolved.default_branch,
          "name": resolved.name,
      }
    output: resolved_repo

  # ==================== QUERY JIRA FOR CVES ====================

  - name: search_cves
    description: "Search for CVEs assigned to downstream component"
    tool: jira_search
    args:
      jql: '"Downstream Component Name" ~ "{{ inputs.downstream_component }}" AND type = Vulnerability AND resolution = Unresolved ORDER BY created DESC'
      max_results: 50
    output: cve_search_raw
    on_error: auto_heal

  - name: parse_cve_results
    description: "Parse CVE search results and extract issue keys"
    compute: |
      import re

      cve_text = str(cve_search_raw) if cve_search_raw else ""

      # Extract issue keys (AAP-XXXXX pattern)
      issue_keys = re.findall(r'(AAP-\d+)', cve_text)

      # Deduplicate while preserving order
      seen = set()
      unique_keys = []
      for key in issue_keys:
          if key not in seen:
              seen.add(key)
              unique_keys.append(key)

      result = {
          "issue_keys": unique_keys,
          "total_found": len(unique_keys),
      }
    output: cve_issues

  # ==================== CHECK ORIGIN/MAIN, BRANCHES, AND MRs ====================

  - name: fetch_origin
    description: "Fetch latest from origin to ensure we have up-to-date refs"
    tool: git_fetch
    args:
      repo: "{{ resolved_repo.path }}"
      prune: true
    output: fetch_result
    on_error: auto_heal

  - name: get_main_log
    description: "Get git log from origin/main to check for merged CVE fixes"
    compute: |
      import subprocess

      repo_path = resolved_repo["path"]
      default_branch = resolved_repo.get("default_branch", "main")

      # Get commits from origin/main (the upstream branch)
      cmd = ["git", "-C", repo_path, "log", f"origin/{default_branch}", "--oneline", "-500"]
      try:
          result_proc = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
          result = result_proc.stdout if result_proc.returncode == 0 else ""
      except Exception as e:
          result = ""

      result = result
    output: main_log_raw

  - name: get_all_branches
    description: "Get all branches to check for CVE work in progress"
    tool: git_branch_list
    args:
      repo: "{{ resolved_repo.path }}"
      all_branches: true
    output: all_branches_raw
    on_error: auto_heal

  - name: get_open_mrs
    description: "Get all open MRs to check for existing CVE fixes"
    tool: gitlab_mr_list
    args:
      project: "{{ resolved_repo.gitlab_project }}"
    output: open_mrs_raw
    on_error: auto_heal

  - name: filter_unfixed_cves
    description: "Filter CVEs by status: merged, has MR, has branch (resumable), or unfixed"
    compute: |
      import re

      # Defensive access - variables may be None if auto_heal failed upstream
      try:
          main_log = str(main_log_raw) if main_log_raw else ""
      except NameError:
          main_log = ""
      try:
          all_branches = str(all_branches_raw) if all_branches_raw else ""
      except NameError:
          all_branches = ""
      try:
          open_mrs = str(open_mrs_raw) if open_mrs_raw else ""
      except NameError:
          open_mrs = ""
      all_cves = cve_issues.get("issue_keys", [])

      unfixed = []
      merged_to_main = []
      has_branch = []
      has_mr = []
      resumable = []  # CVEs with branches but no MR - can resume

      for issue_key in all_cves:
          issue_key_lower = issue_key.lower()

          # Check 1: Is there a commit on origin/main mentioning this issue key?
          # This means the fix is actually merged and deployed
          if issue_key in main_log or issue_key_lower in main_log.lower():
              merged_to_main.append({"key": issue_key, "reason": "merged to main"})
              continue

          # Check 2: Is there an open MR for this issue?
          # Check MRs before branches since MR is more definitive
          if issue_key in open_mrs or issue_key_lower in open_mrs.lower():
              has_mr.append({"key": issue_key, "reason": "MR exists"})
              continue

          # Check 3: Is there a branch for this issue (but no MR yet)?
          # These are RESUMABLE - the skill started but didn't finish
          if issue_key_lower in all_branches.lower():
              has_branch.append({"key": issue_key, "reason": "branch exists"})
              resumable.append(issue_key)
              continue

          # Not found anywhere - this CVE needs work from scratch
          unfixed.append(issue_key)

      # Apply max_cves limit
      # Prioritize resumable CVEs (closer to completion) over brand new ones
      max_cves = inputs.get("max_cves", 1)
      to_process = resumable[:max_cves]
      remaining_slots = max_cves - len(to_process)
      if remaining_slots > 0:
          to_process.extend(unfixed[:remaining_slots])

      result = {
          "to_process": to_process,
          "unfixed_count": len(unfixed),
          "resumable": resumable,
          "resumable_count": len(resumable),
          "merged_to_main": [item["key"] for item in merged_to_main],
          "merged_count": len(merged_to_main),
          "has_branch": [item["key"] for item in has_branch],
          "has_branch_count": len(has_branch),
          "has_mr": [item["key"] for item in has_mr],
          "has_mr_count": len(has_mr),
      }
    output: filtered_cves

  - name: check_has_work
    description: "Check if there are CVEs to process (including resumable ones)"
    compute: |
      to_process = filtered_cves.get("to_process", [])
      resumable = filtered_cves.get("resumable", [])

      if not to_process:
          msg_parts = ["No CVEs to process."]
          if filtered_cves.get("merged_count", 0) > 0:
              msg_parts.append(f"Merged: {filtered_cves['merged_count']}")
          if filtered_cves.get("has_mr_count", 0) > 0:
              msg_parts.append(f"Has MR: {filtered_cves['has_mr_count']}")

          result = {
              "has_work": False,
              "is_resume": False,
              "message": " ".join(msg_parts)
          }
      else:
          issue_key = to_process[0]
          is_resume = issue_key in resumable

          result = {
              "has_work": True,
              "is_resume": is_resume,
              "issue_key": issue_key,
              "message": f"{'Resuming' if is_resume else 'Processing'} {issue_key} ({len(to_process)} CVE(s) total, {len(resumable)} resumable)"
          }
    output: work_check

  # ==================== CHECK RESUME STATE (for existing branches) ====================

  - name: check_resume_state
    description: "For resumed CVEs, find the existing branch, check it out, and detect what work is already done"
    condition: "work_check.has_work and work_check.is_resume"
    compute: |
      import re
      import subprocess

      repo_path = resolved_repo["path"]
      default_branch = resolved_repo.get("default_branch", "main")
      issue_key = work_check["issue_key"]
      issue_key_lower = issue_key.lower()

      # Defensive access for all_branches_raw
      try:
          branches_text = str(all_branches_raw) if all_branches_raw else ""
      except NameError:
          branches_text = ""

      # Find the actual branch name by searching for the issue key in branches
      existing_branch = None
      for line in branches_text.split("\n"):
          line_stripped = line.strip().lstrip("* ")
          # Remove remote prefixes: "remotes/origin/", "origin/", "remotes/fork/", etc.
          branch_name_candidate = re.sub(r'^(?:remotes/)?(?:origin|fork)/', '', line_stripped)
          # Extract just the branch name (before any whitespace or arrow)
          branch_name_candidate = branch_name_candidate.split()[0] if branch_name_candidate.split() else ""
          # Remove backticks that git_branch_list might add
          branch_name_candidate = branch_name_candidate.strip("`")
          if issue_key_lower in branch_name_candidate.lower():
              existing_branch = branch_name_candidate
              break

      if not existing_branch:
          raise ValueError(
              f"Could not find branch for {issue_key} in branch list. "
              "Branch may have been deleted. Will need to create a new one."
          )

      # Check out the existing branch
      checkout_result = subprocess.run(
          ["git", "-C", repo_path, "checkout", existing_branch],
          capture_output=True, text=True, timeout=30,
      )
      if checkout_result.returncode != 0:
          # Try fetching and checking out from remote
          subprocess.run(
              ["git", "-C", repo_path, "checkout", "-b", existing_branch, f"origin/{existing_branch}"],
              capture_output=True, text=True, timeout=30,
          )

      # Check if there are commits on this branch beyond origin/main
      log_result = subprocess.run(
          ["git", "-C", repo_path, "log", f"origin/{default_branch}..HEAD", "--oneline"],
          capture_output=True, text=True, timeout=30,
      )
      commits_ahead = log_result.stdout.strip().split("\n") if log_result.stdout.strip() else []
      has_commits = len(commits_ahead) > 0 and commits_ahead[0] != ""

      # Check if Pipfile/Pipfile.lock are modified in those commits
      already_committed = False
      if has_commits:
          diff_result = subprocess.run(
              ["git", "-C", repo_path, "diff", "--name-only", f"origin/{default_branch}..HEAD"],
              capture_output=True, text=True, timeout=30,
          )
          changed_files = diff_result.stdout.strip().split("\n") if diff_result.stdout.strip() else []
          already_committed = "Pipfile" in changed_files or "Pipfile.lock" in changed_files

      # Check if the branch exists on the remote (already pushed)
      remote_check = subprocess.run(
          ["git", "-C", repo_path, "ls-remote", "--heads", "origin", existing_branch],
          capture_output=True, text=True, timeout=30,
      )
      already_pushed = bool(remote_check.stdout.strip())

      result = {
          "is_resume": True,
          "existing_branch_name": existing_branch,
          "has_commits": has_commits,
          "commits_ahead": len(commits_ahead) if has_commits else 0,
          "already_committed": already_committed,
          "already_pushed": already_pushed,
      }
    output: resume_state
    on_error: continue

  - name: init_resume_state_default
    description: "Initialize resume_state for non-resume (new CVE) flow"
    condition: "work_check.has_work and not work_check.is_resume"
    compute: |
      result = {
          "is_resume": False,
          "existing_branch_name": None,
          "has_commits": False,
          "commits_ahead": 0,
          "already_committed": False,
          "already_pushed": False,
      }
    output: resume_state

  # ==================== PROCESS FIRST CVE ====================

  - name: get_cve_details
    description: "Get detailed CVE information from Jira"
    condition: "work_check.has_work"
    tool: jira_view_issue
    args:
      issue_key: "{{ work_check.issue_key }}"
    output: cve_details_raw
    on_error: auto_heal

  - name: parse_cve_details
    description: "Extract CVE ID and affected package from issue"
    condition: "work_check.has_work"
    compute: |
      import re

      details = str(cve_details_raw) if cve_details_raw else ""
      issue_key = work_check["issue_key"]

      # Extract CVE ID (CVE-YYYY-NNNNN pattern)
      cve_match = re.search(r'(CVE-\d{4}-\d+)', details, re.IGNORECASE)
      cve_id = cve_match.group(1).upper() if cve_match else ""

      # Extract affected package - try multiple strategies
      affected_package = ""

      # Strategy 1: "Upstream Affected Component" field (structured Jira field)
      package_match = re.search(r'upstream.*affected.*component\s*:\s*(\S+)', details, re.IGNORECASE)
      if package_match:
          affected_package = package_match.group(1).strip()

      # Strategy 2: Extract from summary line format:
      #   "CVE-XXXX-NNNNN downstream-component: Package-Name has/is/vulnerable..."
      # The package name follows the colon and precedes description verbs.
      # Must handle hyphenated names like "Python-Multipart", "aiohttp-cors", etc.
      if not affected_package:
          summary_match = re.search(
              r'summary\s*:\s*CVE-\d+-\d+\s+[\w-]+:\s*([\w][\w.-]*(?:-[\w.]+)*)',
              details, re.IGNORECASE
          )
          if summary_match:
              candidate = summary_match.group(1).lower()
              # Filter out common stop words that aren't package names
              if candidate not in ['the', 'a', 'an', 'http', 'is', 'vulnerable']:
                  affected_package = candidate

      # Strategy 3: Look for known Python packages in description
      # Common patterns: "PACKAGE is an HTTP", "PACKAGE's HTTP Parser", "PACKAGE vulnerable"
      if not affected_package:
          known_packages = [
              'python-multipart', 'aiohttp', 'urllib3', 'requests', 'django', 'flask',
              'cryptography', 'pillow', 'numpy', 'pandas', 'jinja2', 'pyyaml', 'sqlalchemy',
              'celery', 'redis', 'psycopg2', 'boto3', 'paramiko', 'lxml', 'multipart',
              'starlette', 'fastapi', 'gunicorn', 'uvicorn', 'httpx', 'certifi',
              'setuptools', 'pip', 'wheel', 'idna', 'charset-normalizer',
          ]
          details_lower = details.lower()
          for pkg in known_packages:
              if re.search(rf'\b{re.escape(pkg)}\b.*(?:is|has|vulnerable|version|prior)', details_lower):
                  affected_package = pkg
                  break

      # Strategy 4: Extract from description - "Package-Name is/has" at start of description paragraph
      # Handles capitalized names like "Python-Multipart is a streaming multipart parser"
      if not affected_package:
          desc_match = re.search(
              r'(?:DESCRIPTION\s*[-=]+\s*.*?\n\n|Flaw:\s*\n+)([\w][\w.-]*(?:-[\w.]+)*)\s+(?:is|has|was|are|were|allows?)\b',
              details, re.IGNORECASE | re.DOTALL
          )
          if desc_match:
              candidate = desc_match.group(1).lower()
              if candidate not in ['the', 'a', 'an', 'this', 'it', 'security', 'do', 'not']:
                  affected_package = candidate

      # Strategy 5: Look for "X vulnerable to" pattern in description
      if not affected_package:
          vuln_match = re.search(r'([\w][\w-]*)\s+(?:is\s+)?vulnerable\s+to', details, re.IGNORECASE)
          if vuln_match:
              candidate = vuln_match.group(1).lower()
              if candidate not in ['the', 'a', 'an', 'http', 'is', 'version', 'prior']:
                  affected_package = candidate

      # Normalize package name: pip uses lowercase with hyphens
      if affected_package:
          affected_package = affected_package.lower().replace('_', '-')

      # Extract summary
      summary_match = re.search(r'summary\s*:\s*(.+?)(?:\n|$)', details, re.IGNORECASE)
      summary = summary_match.group(1).strip()[:100] if summary_match else ""

      # Extract CVSS score if available
      cvss_match = re.search(r'cvss.*score\s*:\s*([\d.]+)', details, re.IGNORECASE)
      cvss_score = cvss_match.group(1) if cvss_match else ""

      # Extract severity/priority
      severity_match = re.search(r'(?:severity|priority)\s*:\s*(\w+)', details, re.IGNORECASE)
      severity = severity_match.group(1) if severity_match else ""

      result = {
          "issue_key": issue_key,
          "cve_id": cve_id,
          "affected_package": affected_package,
          "summary": summary,
          "cvss_score": cvss_score,
          "severity": severity,
          "valid": bool(cve_id and affected_package),
      }
    output: cve_info

  - name: validate_cve_info
    description: "Ensure we have required CVE information and package is pip-installable"
    condition: "work_check.has_work"
    compute: |
      if not cve_info.get("valid"):
          missing = []
          if not cve_info.get("cve_id"):
              missing.append("CVE ID")
          if not cve_info.get("affected_package"):
              missing.append("affected package")

          result = {
              "valid": False,
              "reason": f"Missing required CVE information for {cve_info.get('issue_key')}: {', '.join(missing)}. "
                        "Please ensure the Jira issue has CVE ID and Upstream Affected Component fields populated.",
          }
      else:
          # Reject packages that are not pip-installable (runtimes, system packages, etc.)
          NON_PIP_PACKAGES = {
              "python", "python3", "cpython", "pypy",    # Python runtimes
              "linux", "linux-kernel", "kernel",          # OS kernel
              "glibc", "libc", "openssl-libs",            # System libraries
              "java", "openjdk", "nodejs", "node",        # Other runtimes
              "gcc", "binutils", "gdb",                   # Toolchain
          }
          package = cve_info.get("affected_package", "").lower()
          if package in NON_PIP_PACKAGES:
              result = {
                  "valid": False,
                  "reason": f"Package '{package}' is not a pip-installable Python package (it's a runtime/system package). "
                            f"CVE {cve_info.get('cve_id')} for {cve_info.get('issue_key')} requires a different remediation approach "
                            "(e.g., updating the base container image or system packages).",
              }
          else:
              result = {"valid": True, "reason": "CVE info validated"}
    output: validation_result

  - name: check_compatibility_requirements
    description: "Check for known compatibility requirements when upgrading this package"
    condition: "work_check.has_work and validation_result.valid"
    compute: |
      # Known compatibility requirements discovered from production issues
      # Add new entries here as they are discovered
      COMPATIBILITY_MAP = {
          "urllib3": {
              "min_version": "2.0.0",
              "requires": [
                  {"package": "boto3", "min_version": "1.34.46", "reason": "urllib3 2.x removed DEFAULT_CIPHERS"},
                  {"package": "botocore", "min_version": "1.34.46", "reason": "urllib3 2.x removed DEFAULT_CIPHERS"},
              ],
              "notes": "urllib3 2.x has breaking changes that affect boto3/botocore. Older versions import DEFAULT_CIPHERS which was removed.",
          },
          "aiohttp": {
              "min_version": "3.9.0",
              "requires": [
                  {"package": "gql", "min_version": "3.5.0", "reason": "aiohttp 3.9+ changed AIOHTTPTransport API"},
              ],
              "notes": "aiohttp 3.9+ changed transport APIs. gql < 3.5.0 uses old API and will fail to import.",
          },
          "gql": {
              "min_version": "3.5.0",
              "requires": [
                  {"package": "backoff", "min_version": "2.2.1", "reason": "gql 3.5+ requires backoff for retry logic"},
                  {"package": "requests-toolbelt", "min_version": "1.0.0", "reason": "gql[requests] requires requests-toolbelt"},
              ],
              "notes": "gql 3.5+ added new dependencies. If using gql[requests] extra, requests-toolbelt is required.",
          },
      }

      package = cve_info["affected_package"].lower()
      compat_info = COMPATIBILITY_MAP.get(package, {})

      companion_packages = []
      compatibility_notes = ""

      if compat_info:
          companion_packages = compat_info.get("requires", [])
          compatibility_notes = compat_info.get("notes", "")

      result = {
          "package": package,
          "has_compatibility_requirements": bool(companion_packages),
          "companion_packages": companion_packages,
          "notes": compatibility_notes,
      }
    output: compatibility_check

  # ==================== ASSIGN AND TRANSITION ====================

  - name: resolve_jira_username
    description: "Get actual Jira username (email format) from config"
    condition: "work_check.has_work and not inputs.dry_run and validation_result.valid"
    compute: |
      from scripts.common.config_loader import load_config
      config = load_config()
      user = config.get("user", {})
      # Red Hat Jira requires email-format username (daoneill@redhat.com), not short form
      result = user.get("email", user.get("jira_username", user.get("username", "")))
    output: jira_username

  - name: assign_to_me
    description: "Assign CVE issue to current user"
    condition: "work_check.has_work and not inputs.dry_run and validation_result.valid and jira_username"
    tool: jira_assign
    args:
      issue_key: "{{ cve_info.issue_key }}"
      assignee: "{{ jira_username }}"
    output: assign_result
    on_error: auto_heal

  - name: check_and_set_acceptance_criteria
    description: "Verify acceptance criteria is set (required by Jira workflow for transition)"
    condition: "work_check.has_work and not inputs.dry_run and validation_result.valid"
    compute: |
      import subprocess

      issue_key = cve_info["issue_key"]
      cve_id = cve_info["cve_id"]
      package = cve_info["affected_package"]

      # Check if acceptance criteria is already set by looking at the issue details
      details = str(cve_details_raw) if cve_details_raw else ""
      has_ac = "acceptance criteria" in details.lower() and any(
          line.strip() and not line.strip().startswith("acceptance criteria")
          for line in details.lower().split("acceptance criteria")[-1].split("\n")[:5]
          if line.strip() and line.strip() not in (":", "none", "n/a", "")
      )

      if not has_ac:
          # Build acceptance criteria appropriate for a CVE fix
          ac_text = (
              f"* {cve_id} vulnerability in {package} is remediated\\n"
              f"* {package} is updated to a version that fixes {cve_id}\\n"
              f"* Pipfile and Pipfile.lock are updated with the new version\\n"
              f"* No regressions in existing functionality\\n"
              f"* CI pipeline passes"
          )

          # Use cmd_result (not ac_result) to avoid collision with output variable name
          cmd_result = subprocess.run(
              ["rh-issue", "set-acceptance-criteria", issue_key, ac_text],
              capture_output=True, text=True, timeout=60,
          )

          if cmd_result.returncode == 0:
              result = {"set": True, "message": f"Acceptance criteria set for {issue_key}"}
          else:
              result = {"set": False, "message": f"Failed to set AC: {cmd_result.stderr[:200]}"}
      else:
          result = {"set": False, "message": "Acceptance criteria already present"}
    output: ac_result

  - name: transition_to_progress
    description: "Transition issue to In Progress"
    condition: "work_check.has_work and not inputs.dry_run and validation_result.valid"
    tool: jira_transition
    args:
      issue_key: "{{ cve_info.issue_key }}"
      status: "In Progress"
    output: transition_result
    on_error: continue

  # ==================== CREATE BRANCH ====================

  - name: create_branch_name
    description: "Generate or resolve branch name for CVE fix"
    condition: "work_check.has_work and validation_result.valid"
    compute: |
      from scripts.common.parsers import slugify_text

      # When resuming, use the existing branch name found by check_resume_state
      if work_check.get("is_resume") and isinstance(resume_state, dict) and resume_state.get("existing_branch_name"):
          result = resume_state["existing_branch_name"]
      else:
          issue_key = cve_info["issue_key"]
          cve_id = cve_info["cve_id"].lower()
          package = cve_info["affected_package"].lower().replace("_", "-")

          # Branch format: AAP-XXXXX-cve-2025-69223-aiohttp
          result = f"{issue_key}-{cve_id}-{package}"
    output: branch_name

  - name: fetch_latest
    description: "Fetch latest from origin"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    tool: git_fetch
    args:
      repo: "{{ resolved_repo.path }}"
      prune: true
    output: fetch_latest_result
    on_error: auto_heal

  - name: verify_clean_state
    description: "Verify working directory is clean before creating branch"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    compute: |
      import subprocess

      repo_path = resolved_repo["path"]

      # Check for uncommitted changes
      status_result = subprocess.run(
          ["git", "-C", repo_path, "status", "--porcelain"],
          capture_output=True,
          text=True,
          timeout=30,
      )

      uncommitted = status_result.stdout.strip()
      if uncommitted:
          # List the files that have changes
          changed_files = [line.split()[-1] for line in uncommitted.split("\n") if line]
          raise ValueError(
              f"Working directory has uncommitted changes. Please commit or stash before running CVE fix.\n"
              f"Changed files: {', '.join(changed_files[:5])}"
              f"{'...' if len(changed_files) > 5 else ''}"
          )

      result = "Working directory is clean"
    output: clean_state_check

  - name: checkout_main
    description: "Checkout main branch (skipped when resuming - already on feature branch)"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    tool: git_checkout
    args:
      repo: "{{ resolved_repo.path }}"
      target: "{{ resolved_repo.default_branch }}"
    output: checkout_main_result
    on_error: continue

  - name: reset_to_origin_main
    description: "Reset local main to origin/main to ensure clean base (CRITICAL for branch isolation)"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    compute: |
      import subprocess

      repo_path = resolved_repo["path"]
      default_branch = resolved_repo.get("default_branch", "main")

      # Hard reset to origin/main to ensure we're starting from the latest upstream
      # This prevents cross-contamination from local commits on main
      reset_result = subprocess.run(
          ["git", "-C", repo_path, "reset", "--hard", f"origin/{default_branch}"],
          capture_output=True,
          text=True,
          timeout=30,
      )

      if reset_result.returncode != 0:
          raise ValueError(f"Failed to reset to origin/{default_branch}: {reset_result.stderr}")

      result = f"Reset to origin/{default_branch}"
    output: reset_result

  - name: create_branch
    description: "Create feature branch for CVE fix FROM origin/main (skipped when resuming)"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    tool: git_branch_create
    args:
      repo: "{{ resolved_repo.path }}"
      branch_name: "{{ branch_name }}"
      checkout: true
    output: branch_result
    on_error: continue

  - name: verify_branch_base
    description: "Verify the new branch is based on origin/main (not another feature branch)"
    condition: "work_check.has_work and not inputs.dry_run and not work_check.is_resume and validation_result.valid"
    compute: |
      import subprocess

      repo_path = resolved_repo["path"]
      default_branch = resolved_repo.get("default_branch", "main")

      # Get the merge base between current branch and origin/main
      merge_base_result = subprocess.run(
          ["git", "-C", repo_path, "merge-base", "HEAD", f"origin/{default_branch}"],
          capture_output=True,
          text=True,
          timeout=30,
      )

      # Get the commit hash of origin/main
      origin_main_result = subprocess.run(
          ["git", "-C", repo_path, "rev-parse", f"origin/{default_branch}"],
          capture_output=True,
          text=True,
          timeout=30,
      )

      merge_base = merge_base_result.stdout.strip()
      origin_main = origin_main_result.stdout.strip()

      if merge_base != origin_main:
          raise ValueError(
              f"Branch is not based on origin/{default_branch}! "
              f"Merge base: {merge_base[:8]}, origin/{default_branch}: {origin_main[:8]}. "
              "This could cause cross-contamination with other feature branches."
          )

      result = f"Branch correctly based on origin/{default_branch} ({origin_main[:8]})"
    output: branch_base_verified

  # ==================== UPDATE PIPFILE AND PIPFILE.LOCK ====================

  # ---- Step 1: Prepare temp workspace with Pipfile + Containerfile ----
  - name: prepare_pipenv_workspace
    description: "Create temp directory with minimal Pipfile and Containerfile for container-based pipenv lock"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and validation_result.valid"
    compute: |
      import json
      import os
      import re
      import tempfile

      repo_path = resolved_repo["path"]
      package = cve_info["affected_package"]
      cve_id = cve_info["cve_id"]

      # Get companion packages from compatibility check
      companion_packages = compatibility_check.get("companion_packages", []) if isinstance(compatibility_check, dict) else []

      # Read Python version from the ORIGINAL Pipfile (not Pipfile.lock)
      pipfile_path = os.path.join(repo_path, "Pipfile")
      pipfile_lock_path = os.path.join(repo_path, "Pipfile.lock")

      if not os.path.exists(pipfile_path):
          raise ValueError(f"Pipfile not found at {pipfile_path}")
      if not os.path.exists(pipfile_lock_path):
          raise ValueError(f"Pipfile.lock not found at {pipfile_lock_path}")

      with open(pipfile_path, "r") as f:
          original_pipfile = f.read()

      # Extract Python version from Pipfile's [requires] section
      python_version_match = re.search(r'python_version\s*=\s*["\']([^"\']+)["\']', original_pipfile)
      if not python_version_match:
          raise ValueError("Could not find python_version in Pipfile [requires] section")
      project_python_version = python_version_match.group(1)
      python_version = project_python_version

      # Map Python version to UBI container image
      PYTHON_IMAGE_MAP = {
          "3.9":  "registry.access.redhat.com/ubi9/python-39",
          "3.11": "registry.access.redhat.com/ubi9/python-311",
          "3.12": "registry.access.redhat.com/ubi9/python-312",
      }
      container_image = PYTHON_IMAGE_MAP.get(
          python_version,
          PYTHON_IMAGE_MAP["3.9"],
      )
      container_tag = f"cve-fix-python{python_version.replace('.', '')}"

      # Read current Pipfile.lock for companion package detection
      with open(pipfile_lock_path, "r") as f:
          current_lock = json.load(f)

      # Create temp directory for isolated pipenv (SELinux-friendly perms)
      temp_dir = tempfile.mkdtemp(prefix="cve-fix-", dir="/tmp")
      os.chmod(temp_dir, 0o777)

      # Build package list: main package + any companion packages
      packages_to_install = [f'{package} = "*"']
      companion_updates = []
      for comp in companion_packages:
          comp_name = comp["package"]
          comp_min_version = comp["min_version"]
          comp_reason = comp["reason"]

          # Check if companion package exists in current lock
          comp_in_lock = False
          for section in ["default", "develop"]:
              if section in current_lock:
                  for pkg_name in current_lock[section].keys():
                      if pkg_name.lower() == comp_name.lower():
                          comp_in_lock = True
                          break
              if comp_in_lock:
                  break

          if comp_in_lock:
              packages_to_install.append(f'{comp_name} = ">={comp_min_version}"')
              companion_updates.append({
                  "package": comp_name,
                  "min_version": comp_min_version,
                  "reason": comp_reason,
              })

      # Write minimal Pipfile for the container
      temp_pipfile_lines = [
          "[[source]]",
          'url = "https://pypi.org/simple"',
          "verify_ssl = true",
          'name = "pypi"',
          "",
          "[packages]",
      ]
      temp_pipfile_lines.extend(packages_to_install)
      temp_pipfile_lines.extend([
          "",
          "[requires]",
          f'python_version = "{python_version}"',
      ])
      with open(os.path.join(temp_dir, "Pipfile"), "w") as f:
          f.write("\n".join(temp_pipfile_lines) + "\n")

      # Write Containerfile for the pipenv lock environment
      with open(os.path.join(temp_dir, "Containerfile"), "w") as f:
          f.write(f"FROM {container_image}\n")
          f.write("USER 0\n")
          f.write("RUN pip install --upgrade pip pipenv\n")
          f.write("WORKDIR /work\n")

      result = {
          "temp_dir": temp_dir,
          "container_image": container_image,
          "container_tag": container_tag,
          "python_version": python_version,
          "project_python_version": project_python_version,
          "original_pipfile": original_pipfile,
          "companion_updates": companion_updates,
      }
    output: pipenv_workspace

  # ---- Step 2: Build the container image with pipenv installed ----
  - name: build_pipenv_image
    description: "Build container image with Python + pipenv from UBI base"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and validation_result.valid and pipenv_workspace is mapping"
    tool: podman_build
    args:
      repo: "{{ pipenv_workspace.temp_dir }}"
      tag: "{{ pipenv_workspace.container_tag }}"
      dockerfile: "Containerfile"
      timeout: 300
    output: container_build_result
    on_error: continue

  # ---- Step 3: Run pipenv lock inside the container ----
  - name: run_pipenv_lock
    description: "Run pipenv lock in container with host-mounted temp dir to resolve package versions"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and validation_result.valid and pipenv_workspace is mapping and container_build_result is string and '❌' not in container_build_result"
    tool: podman_run
    args:
      image: "{{ pipenv_workspace.container_tag }}"
      command: "pipenv lock"
      detach: false
      rm: true
      volumes: "{{ pipenv_workspace.temp_dir }}:/work:Z"
      env_vars: "PIPENV_IGNORE_VIRTUALENVS=1 PIPENV_VENV_IN_PROJECT=1"
      timeout: 180
    output: pipenv_lock_result
    on_error: continue

  # ---- Step 4: Read lock results and update Pipfile + Pipfile.lock in the repo ----
  - name: update_pipfile_and_lock
    description: "Read container-generated Pipfile.lock and merge new versions/hashes into the project files"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and validation_result.valid and pipenv_workspace is mapping and pipenv_lock_result is string and '❌' not in pipenv_lock_result"
    compute: |
      import json
      import os
      import re

      repo_path = resolved_repo["path"]
      package = cve_info["affected_package"]
      cve_id = cve_info["cve_id"]

      temp_dir = pipenv_workspace["temp_dir"]
      python_version = pipenv_workspace["python_version"]
      project_python_version = pipenv_workspace["project_python_version"]
      container_image = pipenv_workspace["container_image"]
      original_pipfile = pipenv_workspace["original_pipfile"]
      companion_updates = pipenv_workspace["companion_updates"]

      pipfile_path = os.path.join(repo_path, "Pipfile")
      pipfile_lock_path = os.path.join(repo_path, "Pipfile.lock")

      # Read the new lock file generated by the container
      new_lock_path = os.path.join(temp_dir, "Pipfile.lock")
      if not os.path.exists(new_lock_path):
          raise ValueError(f"Container did not produce Pipfile.lock at {new_lock_path}")
      with open(new_lock_path, "r") as f:
          new_lock = json.load(f)

      # Read current project Pipfile.lock
      with open(pipfile_lock_path, "r") as f:
          current_lock = json.load(f)

      # ==================== EXTRACT PACKAGE INFO FROM TEMP LOCK ====================
      def get_package_info(lock_data, pkg_name):
          """Extract package info from lock file."""
          pkg_lower = pkg_name.lower()
          for section in ["default", "develop"]:
              if section in lock_data:
                  for name, info in lock_data[section].items():
                      if name.lower() == pkg_lower:
                          return info
          return None

      # Get main package info
      new_package_info = get_package_info(new_lock, package)
      if not new_package_info:
          raise ValueError(f"Package {package} not found in temp Pipfile.lock")

      new_version = new_package_info.get("version", "")
      new_hashes = new_package_info.get("hashes", [])
      version_num = new_version.lstrip("=<>~!")

      # Get companion package info
      companion_info = []
      for comp in companion_updates:
          comp_pkg_info = get_package_info(new_lock, comp["package"])
          if comp_pkg_info:
              companion_info.append({
                  "package": comp["package"],
                  "version": comp_pkg_info.get("version", ""),
                  "hashes": comp_pkg_info.get("hashes", []),
                  "min_version": comp["min_version"],
                  "reason": comp["reason"],
              })

      # ==================== UPDATE PIPFILE ====================
      updated_pipfile = original_pipfile

      # Update main package
      package_pattern = rf'^{re.escape(package)}\s*=.*$'
      package_exists = re.search(package_pattern, updated_pipfile, re.MULTILINE | re.IGNORECASE)

      if package_exists:
          new_line = f'{package} = ">={version_num}"  # {cve_id}'
          updated_pipfile = re.sub(
              package_pattern,
              new_line,
              updated_pipfile,
              flags=re.MULTILINE | re.IGNORECASE
          )
      else:
          packages_match = re.search(r'(\[packages\].*?\n)', updated_pipfile, re.DOTALL)
          if packages_match:
              insert_pos = packages_match.end()
              new_line = f'{package} = ">={version_num}"  # {cve_id}\n'
              updated_pipfile = updated_pipfile[:insert_pos] + new_line + updated_pipfile[insert_pos:]
          else:
              raise ValueError("Could not find [packages] section in Pipfile")

      # Update companion packages in Pipfile
      for comp in companion_info:
          comp_name = comp["package"]
          comp_version = comp["version"].lstrip("=<>~!")
          comp_reason = comp["reason"]

          comp_pattern = rf'^{re.escape(comp_name)}\s*=.*$'
          comp_exists = re.search(comp_pattern, updated_pipfile, re.MULTILINE | re.IGNORECASE)

          if comp_exists:
              new_comp_line = f'{comp_name} = ">={comp_version}"  # {comp_reason}'
              updated_pipfile = re.sub(
                  comp_pattern,
                  new_comp_line,
                  updated_pipfile,
                  flags=re.MULTILINE | re.IGNORECASE
              )

      # Write updated Pipfile
      with open(pipfile_path, "w") as f:
          f.write(updated_pipfile)

      # ==================== UPDATE PIPFILE.LOCK ====================
      def update_lock_package(lock_data, pkg_name, new_ver, new_hash):
          """Update a package in the lock file, return old version."""
          pkg_lower = pkg_name.lower()
          for section in ["default", "develop"]:
              if section in lock_data:
                  for name in list(lock_data[section].keys()):
                      if name.lower() == pkg_lower:
                          old_ver = lock_data[section][name].get("version", "")
                          lock_data[section][name]["version"] = new_ver
                          lock_data[section][name]["hashes"] = new_hash
                          return old_ver
          return None

      # Update main package
      old_version = update_lock_package(current_lock, package, new_version, new_hashes)
      if old_version is None:
          if "default" not in current_lock:
              current_lock["default"] = {}
          current_lock["default"][package] = {
              "version": new_version,
              "hashes": new_hashes,
          }
          old_version = "(not present)"

      # Update companion packages
      companion_changes = []
      for comp in companion_info:
          comp_old = update_lock_package(
              current_lock,
              comp["package"],
              comp["version"],
              comp["hashes"]
          )
          if comp_old:
              companion_changes.append({
                  "package": comp["package"],
                  "old_version": comp_old,
                  "new_version": comp["version"],
                  "reason": comp["reason"],
              })

      # Write updated Pipfile.lock
      with open(pipfile_lock_path, "w") as f:
          json.dump(current_lock, f, indent=4)
          f.write("\n")

      result = {
          "success": True,
          "package": package,
          "old_version": old_version,
          "new_version": new_version,
          "version_constraint": f">={version_num}",
          "hashes_count": len(new_hashes),
          "python_version": python_version,
          "project_python_version": project_python_version,
          "python_fallback": False,
          "container_image": container_image,
          "temp_dir": temp_dir,
          "pipfile_updated": True,
          "pipfile_lock_updated": True,
          "companion_packages_updated": companion_changes,
          "compatibility_notes": compatibility_check.get("notes", "") if isinstance(compatibility_check, dict) else "",
      }
    output: pipfile_update

  - name: cleanup_temp
    description: "Clean up temporary directory"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed"
    compute: |
      import shutil

      # Try pipfile_update first (set by final step), fall back to pipenv_workspace (set by prep step)
      temp_dir = ""
      try:
          if isinstance(pipfile_update, dict):
              temp_dir = pipfile_update.get("temp_dir", "")
      except (NameError, TypeError):
          pass
      if not temp_dir:
          try:
              if isinstance(pipenv_workspace, dict):
                  temp_dir = pipenv_workspace.get("temp_dir", "")
          except (NameError, TypeError):
              pass

      if temp_dir and temp_dir.startswith("/tmp/"):
          shutil.rmtree(temp_dir, ignore_errors=True)

      result = "Temp directory cleaned up"
    output: cleanup_result

  # ==================== COMMIT CHANGES ====================

  - name: build_commit_message
    description: "Build commit message for CVE fix"
    condition: "work_check.has_work and validation_result.valid"
    compute: |
      from scripts.common.config_loader import format_commit_message

      issue_key = cve_info["issue_key"]
      cve_id = cve_info["cve_id"]
      package = cve_info["affected_package"]

      # Handle pipfile_update being a dict or string (error case)
      if isinstance(pipfile_update, dict):
          old_version = pipfile_update.get("old_version", "")
          new_version = pipfile_update.get("new_version", "")
      else:
          old_version = ""
          new_version = ""

      description = f"update {package} to fix {cve_id}"
      if old_version and new_version:
          description = f"update {package} {old_version} -> {new_version} to fix {cve_id}"

      message = format_commit_message(
          description=description,
          issue_key=issue_key,
          commit_type="fix",
          scope="deps",
      )

      result = message
    output: commit_message

  - name: stage_changes
    description: "Stage Pipfile and Pipfile.lock changes (skipped if update failed or already committed)"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and pipfile_update is mapping"
    tool: git_add
    args:
      repo: "{{ resolved_repo.path }}"
      files: "Pipfile Pipfile.lock"
    output: stage_result
    on_error: continue

  - name: commit_changes
    description: "Commit the CVE fix (skipped if update failed or already committed)"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_committed and pipfile_update is mapping"
    tool: git_commit
    args:
      repo: "{{ resolved_repo.path }}"
      message: "{{ commit_message }}"
    output: commit_result
    on_error: continue

  # ==================== CREATE MR ====================

  - name: push_branch
    description: "Push branch to origin (skipped when already pushed or update failed)"
    condition: "work_check.has_work and not inputs.dry_run and not resume_state.already_pushed and pipfile_update is mapping"
    tool: git_push
    args:
      repo: "{{ resolved_repo.path }}"
      branch: "{{ branch_name }}"
      set_upstream: true
    output: push_result
    on_error: auto_heal

  - name: build_mr_description
    description: "Build MR description with CVE details and companion package info"
    condition: "work_check.has_work and validation_result.valid"
    compute: |
      import textwrap

      issue_key = cve_info["issue_key"]
      cve_id = cve_info["cve_id"]
      package = cve_info["affected_package"]
      summary = cve_info.get("summary", "")
      cvss_score = cve_info.get("cvss_score", "")
      severity = cve_info.get("severity", "")

      # Handle pipfile_update being a dict or string (error case)
      if isinstance(pipfile_update, dict):
          old_version = pipfile_update.get("old_version", "")
          new_version = pipfile_update.get("new_version", "")
          version_constraint = pipfile_update.get("version_constraint", "")
          companion_changes = pipfile_update.get("companion_packages_updated", [])
          compatibility_notes = pipfile_update.get("compatibility_notes", "")
      else:
          old_version = ""
          new_version = ""
          version_constraint = ""
          companion_changes = []
          compatibility_notes = ""

      # Build changes section
      changes_lines = [
          f"- Added `{package} = \"{version_constraint}\"` to `Pipfile` (pinned to fixed version)",
          f"- Updated `{package}` from `{old_version}` to `{new_version}` in `Pipfile.lock`",
          "- Updated hashes in `Pipfile.lock`",
      ]

      # Add companion package changes if any
      if companion_changes:
          changes_lines.append("")
          changes_lines.append("### Companion Package Updates (for compatibility)")
          changes_lines.append("")
          for comp in companion_changes:
              comp_pkg = comp.get("package", "")
              comp_old = comp.get("old_version", "")
              comp_new = comp.get("new_version", "")
              comp_reason = comp.get("reason", "")
              changes_lines.append(f"- `{comp_pkg}`: `{comp_old}` → `{comp_new}` ({comp_reason})")

      changes_section = "\n".join(changes_lines)

      # Build compatibility notes section if present
      compat_section = ""
      if compatibility_notes:
          compat_section = f"\n## Compatibility Notes\n\n{compatibility_notes}\n"

      description_parts = [
          "## Summary",
          "",
          f"Security fix for {cve_id} in {package}.",
          "",
          summary,
          "",
          "## CVE Details",
          "",
          f"- **CVE ID:** [{cve_id}](https://nvd.nist.gov/vuln/detail/{cve_id})",
          f"- **Package:** {package}",
          f"- **Severity:** {severity}",
          f"- **CVSS Score:** {cvss_score}",
          "",
          "## Changes",
          "",
          changes_section,
          compat_section,
          "## Jira",
          "",
          f"[{issue_key}](https://issues.redhat.com/browse/{issue_key})",
          "",
          "## Testing",
          "",
          "- [ ] CI pipeline passes",
          "- [ ] No breaking changes to existing functionality",
          "- [ ] Local test collection passes (`pytest --collect-only`)",
          "",
          "## Checklist",
          "",
          "- [x] Pipfile updated with version constraint and CVE comment",
          "- [x] Pipfile.lock updated with new version and hashes",
          "- [x] Companion packages updated if needed for compatibility",
          "- [ ] Verified package version addresses CVE",
      ]

      description = "\n".join(description_parts)

      result = description
    output: mr_description

  - name: build_mr_title
    description: "Build MR title"
    condition: "work_check.has_work and validation_result.valid"
    compute: |
      from scripts.common.config_loader import format_commit_message

      issue_key = cve_info["issue_key"]
      cve_id = cve_info["cve_id"]
      package = cve_info["affected_package"]

      title = format_commit_message(
          description=f"fix {cve_id} in {package}",
          issue_key=issue_key,
          commit_type="fix",
          scope="security",
      )

      result = title
    output: mr_title

  - name: create_mr
    description: "Create GitLab merge request (skipped if update failed)"
    condition: "work_check.has_work and not inputs.dry_run and pipfile_update is mapping"
    tool: gitlab_mr_create
    args:
      project: "{{ resolved_repo.gitlab_project }}"
      title: "{{ mr_title }}"
      description: "{{ mr_description }}"
      target_branch: "{{ resolved_repo.default_branch }}"
      source_branch: "{{ branch_name }}"
      draft: false
    output: mr
    on_error: auto_heal

  # ==================== UPDATE JIRA ====================

  - name: build_jira_comment
    description: "Build Jira comment with safe variable access"
    condition: "work_check.has_work and not inputs.dry_run and mr"
    compute: |
      issue_key = cve_info["issue_key"]
      cve_id = cve_info["cve_id"]
      package = cve_info["affected_package"]
      mr_url = mr.get("web_url", "") if isinstance(mr, dict) else str(mr)

      # Handle pipfile_update being a dict or string (error case)
      if isinstance(pipfile_update, dict):
          old_version = pipfile_update.get("old_version", "")
          new_version = pipfile_update.get("new_version", "")
          version_info = f"{old_version} -> {new_version}" if old_version and new_version else "to latest"
      else:
          version_info = "to latest"

      comment = f"MR created to fix {cve_id}:\n{mr_url}\n\nChanges:\n- Updated {package} {version_info}"

      result = {
          "issue_key": issue_key,
          "comment": comment,
      }
    output: jira_comment_data

  - name: add_mr_link_to_jira
    description: "Add MR link to Jira issue"
    condition: "work_check.has_work and not inputs.dry_run and mr and jira_comment_data"
    tool: jira_add_comment
    args:
      issue_key: "{{ jira_comment_data.issue_key }}"
      comment: "{{ jira_comment_data.comment }}"
    output: jira_comment_result
    on_error: auto_heal

  # ==================== NOTIFY TEAM ====================

  - name: build_slack_notification_data
    description: "Build Slack notification data with safe variable access"
    condition: "work_check.has_work and not inputs.dry_run and mr"
    compute: |
      import json

      cve_id = cve_info.get("cve_id", "") if isinstance(cve_info, dict) else ""
      package = cve_info.get("affected_package", "") if isinstance(cve_info, dict) else ""
      severity = cve_info.get("severity", "") if isinstance(cve_info, dict) else ""

      # Handle pipfile_update being a dict or string
      if isinstance(pipfile_update, dict):
          new_version = pipfile_update.get("new_version", "")
      else:
          new_version = ""

      # Handle mr being a dict or string
      # gitlab_mr_create returns a string like "✅ MR Created\n\nhttps://gitlab.cee.redhat.com/.../merge_requests/1509"
      import re as re_mod
      if isinstance(mr, dict):
          mr_url = mr.get("web_url", "")
          mr_id = str(mr.get("iid", ""))
      elif isinstance(mr, str):
          # Extract URL from the string result
          url_match = re_mod.search(r'(https?://\S+merge_requests/(\d+))', str(mr))
          if url_match:
              mr_url = url_match.group(1)
              mr_id = url_match.group(2)
          else:
              mr_url = ""
              mr_id = ""
      else:
          mr_url = ""
          mr_id = ""

      issue_key = cve_info.get("issue_key", "") if isinstance(cve_info, dict) else ""

      notification_inputs = {
          "message": "",
          "template": "cve_fix",
          "template_data": {
              "cve_id": cve_id,
              "package": package,
              "version": new_version,
              "mr_url": mr_url,
              "mr_id": mr_id,
              "severity": severity,
              "issue_key": issue_key,
          }
      }

      result = json.dumps(notification_inputs)
    output: slack_notification_inputs

  - name: notify_team_slack
    description: "Notify team Slack channel about CVE fix"
    condition: "work_check.has_work and not inputs.dry_run and mr and slack_notification_inputs"
    tool: skill_run
    args:
      skill_name: notify_team
      inputs: "{{ slack_notification_inputs }}"
    output: slack_notification
    on_error: continue

  # ==================== MEMORY LOGGING ====================

  - name: log_session
    description: "Log CVE fix to session"
    condition: "work_check.has_work"
    tool: memory_session_log
    args:
      action: "{{ 'Would fix' if inputs.dry_run else 'Fixed' }} CVE {{ cve_info.cve_id }} in {{ cve_info.affected_package }}"
      details: "Issue: {{ cve_info.issue_key }}, MR: {{ mr.web_url if mr else 'N/A (dry run)' }}"
    on_error: continue

  # ==================== ATTACH SESSION CONTEXT TO JIRA ====================

  - name: attach_session_context
    description: "Attach AI session context to CVE Jira issue for security audit trail"
    condition: "work_check.has_work and not inputs.dry_run and mr"
    tool: jira_attach_session
    args:
      issue_key: "{{ cve_info.issue_key }}"
      include_transcript: true
    output: session_context_attached
    on_error: continue

  # ==================== VERIFY FIX WITH VULNERABILITY SCAN ====================

  - name: extract_commit_sha
    description: "Extract commit SHA from git commit result for vulnerability scanning"
    condition: "work_check.has_work and not inputs.dry_run and commit_result"
    compute: |
      import re

      commit_text = str(commit_result) if commit_result else ""

      # Extract SHA from git commit output (format: [branch SHA] message)
      sha_match = re.search(r'([a-f0-9]{7,40})', commit_text)
      fix_sha = sha_match.group(1) if sha_match else None

      result = fix_sha
    output: fix_commit_sha
    on_error: continue

  - name: scan_fixed_image
    description: "Scan the new image to verify CVEs are resolved"
    condition: "work_check.has_work and not inputs.dry_run and fix_commit_sha"
    tool: skill_run
    args:
      skill_name: scan_vulnerabilities
      inputs: '{"image_tag": "{{ fix_commit_sha }}", "repository": "aap-aa-tenant/aap-aa-main/automation-analytics-backend-main", "namespace": "redhat-user-workloads"}'
    output: vuln_scan_result
    on_error: continue

  - name: restore_persona_after_scan
    description: "Restore developer persona after vulnerability scan"
    condition: "work_check.has_work and not inputs.dry_run"
    tool: persona_load
    args:
      persona_name: "developer"
    on_error: continue

outputs:
  - name: summary
    value: |
      ## {{ "🔍 CVE Fix Dry Run" if inputs.dry_run else "✅ CVE Fix Complete" }}

      ### CVE Status Summary

      | Status | Count | Issues |
      |--------|-------|--------|
      | ✅ Merged to main | {{ filtered_cves.merged_count }} | {{ filtered_cves.merged_to_main | join(', ') if filtered_cves.merged_to_main else '-' }} |
      | 🔀 Has Open MR | {{ filtered_cves.has_mr_count }} | {{ filtered_cves.has_mr | join(', ') if filtered_cves.has_mr else '-' }} |
      | 🔄 Resumable (branch, no MR) | {{ filtered_cves.resumable_count }} | {{ filtered_cves.resumable | join(', ') if filtered_cves.resumable else '-' }} |
      | 🔧 Needs Work | {{ filtered_cves.unfixed_count }} | {{ filtered_cves.to_process | join(', ') if filtered_cves.to_process else '-' }} |

      {% if not work_check.has_work %}
      {{ work_check.message }}
      {% elif validation_result is mapping and not validation_result.valid %}
      ---
      ### Skipped: {{ cve_info.issue_key }}

      | Field | Value |
      |-------|-------|
      | Issue | [{{ cve_info.issue_key }}](https://issues.redhat.com/browse/{{ cve_info.issue_key }}) |
      | CVE ID | {{ cve_info.cve_id }} |
      | Package | {{ cve_info.affected_package }} |
      | Severity | {{ cve_info.severity }} |
      | CVSS | {{ cve_info.cvss_score }} |
      | Status | **Skipped** - not pip-installable |

      **Reason:** {{ validation_result.reason }}

      This CVE requires a different remediation approach (e.g., updating the base container image).
      {% else %}
      ---
      ### {{ 'Resuming' if work_check.is_resume else 'Processing' }}: {{ cve_info.issue_key }}

      | Field | Value |
      |-------|-------|
      | Issue | [{{ cve_info.issue_key }}](https://issues.redhat.com/browse/{{ cve_info.issue_key }}) |
      | CVE ID | {{ cve_info.cve_id }} |
      | Package | {{ cve_info.affected_package }} |
      | Severity | {{ cve_info.severity }} |
      | CVSS | {{ cve_info.cvss_score }} |
      {% if work_check.is_resume and resume_state is mapping %}
      | Mode | **Resume** (branch: `{{ resume_state.existing_branch_name }}`) |
      | Commits ahead | {{ resume_state.commits_ahead }} |
      | Pipfile committed | {{ '✅ Yes' if resume_state.already_committed else '❌ No' }} |
      | Already pushed | {{ '✅ Yes' if resume_state.already_pushed else '❌ No' }} |
      {% endif %}

      {% if not inputs.dry_run and pipfile_update is mapping %}
      ### Changes Made

      - **Branch:** `{{ branch_name }}`
      - **Python Version:** `{{ pipfile_update.python_version | default('N/A') }}`
      - **Pipfile:** Added `{{ cve_info.affected_package }} = "{{ pipfile_update.version_constraint | default('>=latest') }}"`
      - **Pipfile.lock:** `{{ pipfile_update.old_version | default('?') }}` → `{{ pipfile_update.new_version | default('?') }}`
      - **Hashes Updated:** {{ pipfile_update.hashes_count | default(0) }}

      {% if pipfile_update.companion_packages_updated %}
      ### Companion Package Updates

      These packages were also updated for compatibility:

      | Package | Old Version | New Version | Reason |
      |---------|-------------|-------------|--------|
      {% for comp in pipfile_update.companion_packages_updated %}
      | {{ comp.package }} | {{ comp.old_version }} | {{ comp.new_version }} | {{ comp.reason }} |
      {% endfor %}

      {% if pipfile_update.compatibility_notes %}
      **Note:** {{ pipfile_update.compatibility_notes }}
      {% endif %}
      {% endif %}

      {% if mr is mapping %}
      ### Merge Request

      **MR:** [{{ mr.title | default('MR') }}]({{ mr.web_url | default('#') }})
      {% endif %}
      {% elif not inputs.dry_run and work_check.is_resume and resume_state is mapping and resume_state.already_committed %}
      ### Resumed from Existing Branch

      - **Branch:** `{{ branch_name }}`
      - **Commits ahead of main:** {{ resume_state.commits_ahead }}
      - **Skipped:** Pipfile update (already committed), staging, commit
      {% if not resume_state.already_pushed %}- **Pushed:** Branch pushed to origin{% endif %}

      {% if mr is mapping %}
      ### Merge Request

      **MR:** [{{ mr.title | default('MR') }}]({{ mr.web_url | default('#') }})
      {% endif %}
      {% elif not inputs.dry_run %}
      ### Changes Made

      - **Branch:** `{{ branch_name }}`
      - **Note:** Pipfile update details unavailable (manual update may have been performed)

      {% if mr is mapping %}
      ### Merge Request

      **MR:** [{{ mr.title | default('MR') }}]({{ mr.web_url | default('#') }})
      {% endif %}
      {% else %}
      ### Would Do

      1. Assign {{ cve_info.issue_key }} to current user
      {% if work_check.is_resume %}
      2. Resume from existing branch: `{{ branch_name }}`
      3. Check existing state (commits, push status)
      4. Skip already-completed steps
      5. Push branch if needed
      6. Create MR with CVE details
      7. Add MR link to Jira issue
      {% else %}
      2. Create branch: `{{ branch_name }}`
      3. Create temp pipenv in /tmp with Python {{ pipfile_update.python_version if pipfile_update is mapping else 'from Pipfile' }}
      4. Install latest {{ cve_info.affected_package }} to get version and hashes
      5. Update Pipfile with version constraint (>= fixed version)
      6. Update Pipfile.lock with new version and hashes
      7. Create MR with CVE details
      8. Add MR link to Jira issue
      {% endif %}
      {% endif %}

      {% if filtered_cves.unfixed_count + filtered_cves.resumable_count > 1 %}
      ---
      **Note:** {{ filtered_cves.unfixed_count + filtered_cves.resumable_count - 1 }} more CVE(s) remaining to process after this one.
      {% endif %}
      {% endif %}

  - name: context
    value:
      processed: "{{ work_check.has_work }}"
      resumed: "{{ work_check.is_resume if work_check else false }}"
      issue_key: "{{ cve_info.issue_key if cve_info else none }}"
      cve_id: "{{ cve_info.cve_id if cve_info else none }}"
      package: "{{ cve_info.affected_package if cve_info else none }}"
      mr_url: "{{ mr.web_url if mr else none }}"
      branch: "{{ branch_name if branch_name else none }}"
      remaining_cves: "{{ (filtered_cves.unfixed_count + filtered_cves.resumable_count - 1) if filtered_cves else 0 }}"
      resumable_count: "{{ filtered_cves.resumable_count if filtered_cves else 0 }}"
