# Skill: Work Analysis
# Analyze work activity across repositories for management reporting
# Categorizes commits by work type and generates effort distribution reports

name: work_analysis
description: |
  Analyze work activity across all configured repositories for a given time period.
  
  This skill:
  - Gathers commits from all repos (excluding redhat-ai-workflow by default)
  - Categorizes work into: DevOps, Development, Testing, Bug Fixes, Documentation,
    Code Review, Incident Response, and Other
  - Pulls Jira data (issues completed, story points)
  - Pulls GitLab MR data (created, reviewed, merged)
  - Generates a markdown report showing effort distribution
  
  Useful for sprint reviews, management reports, and time tracking.

version: "1.0"

inputs:
  - name: start_date
    type: string
    required: false
    default: ""
    description: "Start date in YYYY-MM-DD format (default: 6 months ago)"

  - name: end_date
    type: string
    required: false
    default: ""
    description: "End date in YYYY-MM-DD format (default: today)"

  - name: author
    type: string
    required: false
    default: ""
    description: "Filter by author email (default: current user from config)"

  - name: authors
    type: list
    required: false
    default: []
    description: "List of author emails to include (for multiple accounts). Overrides 'author' if provided."

  - name: repos
    type: list
    required: false
    default: []
    description: "Specific repos to analyze (default: all from config except excluded)"

  - name: exclude_repos
    type: list
    required: false
    default: ["redhat-ai-workflow"]
    description: "Repos to exclude from analysis"

steps:

  # ==================== CONFIGURATION ====================

  - name: load_config
    description: "Load configuration and determine repos to analyze"
    compute: |
      from datetime import datetime, timedelta
      from dateutil.relativedelta import relativedelta
      from scripts.common.config_loader import load_config

      config = load_config()
      user_config = config.get("user", {})
      
      # Get authors - support single author or list of authors
      authors_list = inputs.get("authors", [])
      if authors_list:
          # Use provided list of authors
          authors = authors_list
      else:
          # Fall back to single author or config default
          author = inputs.get("author", "")
          if not author:
              author = user_config.get("email", "")
          authors = [author] if author else []
      
      # Calculate date range - default to 6 months if not provided
      end_date = inputs.get("end_date", "")
      start_date = inputs.get("start_date", "")
      
      if not end_date:
          end_date = datetime.now().strftime("%Y-%m-%d")
      
      if not start_date:
          # Default to 6 months ago
          six_months_ago = datetime.now() - relativedelta(months=6)
          start_date = six_months_ago.strftime("%Y-%m-%d")
      
      # Get all repos from config
      all_repos = config.get("repositories", {})
      
      # Determine which repos to analyze
      requested_repos = inputs.get("repos", [])
      exclude_repos = inputs.get("exclude_repos", ["redhat-ai-workflow"])
      
      repos_to_analyze = []
      if requested_repos:
          # Use specific repos if provided
          for name in requested_repos:
              if name in all_repos and name not in exclude_repos:
                  repo_cfg = all_repos[name]
                  repos_to_analyze.append({
                      "name": name,
                      "path": repo_cfg.get("path", ""),
                      "gitlab": repo_cfg.get("gitlab", ""),
                      "jira_project": repo_cfg.get("jira_project", ""),
                  })
      else:
          # Use all repos except excluded
          for name, repo_cfg in all_repos.items():
              if name not in exclude_repos:
                  repos_to_analyze.append({
                      "name": name,
                      "path": repo_cfg.get("path", ""),
                      "gitlab": repo_cfg.get("gitlab", ""),
                      "jira_project": repo_cfg.get("jira_project", ""),
                  })
      
      result = {
          "config": config,
          "authors": authors,
          "author_name": user_config.get("full_name", "Unknown"),
          "repos": repos_to_analyze,
          "repo_count": len(repos_to_analyze),
          "start_date": start_date,
          "end_date": end_date,
      }
    output: ctx

  # ==================== CATEGORY DEFINITIONS ====================

  - name: define_categories
    description: "Define work category patterns for commit classification"
    compute: |
      import re
      
      # Category patterns - commits can match multiple categories
      categories = {
          "devops": {
              "name": "DevOps",
              "patterns": [
                  r'\b(ci|cd|cicd|ci/cd)\b',
                  r'\b(deploy|deployment|deploying)\b',
                  r'\b(infra|infrastructure)\b',
                  r'\b(pipeline|pipelines)\b',
                  r'\b(k8s|kubernetes|openshift|oc)\b',
                  r'\b(docker|container|podman)\b',
                  r'\b(helm|chart)\b',
                  r'\b(ansible|playbook)\b',
                  r'\b(terraform|tf)\b',
                  r'\b(jenkins|tekton|konflux)\b',
                  r'\b(namespace|cluster)\b',
                  r'\b(saas|app-interface)\b',
                  r'\.gitlab-ci\.yml',
                  r'Dockerfile',
                  r'Makefile',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "development": {
              "name": "Development",
              "patterns": [
                  r'^feat(\(|:|\s)',
                  r'^feature(\(|:|\s)',
                  r'\b(add|adding|added)\s+(new\s+)?(feature|endpoint|api|model|view|component)',
                  r'\b(implement|implementing|implemented)\b',
                  r'\b(enhance|enhancement|enhancing)\b',
                  r'^refactor(\(|:|\s)',
                  r'\b(refactor|refactoring)\b',
                  r'^perf(\(|:|\s)',
                  r'\b(optimize|optimization)\b',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "testing": {
              "name": "Testing",
              "patterns": [
                  r'^test(\(|:|\s)',
                  r'\b(test|tests|testing)\b',
                  r'\b(spec|specs)\b',
                  r'\b(coverage)\b',
                  r'\b(fixture|fixtures)\b',
                  r'\b(mock|mocking|mocked)\b',
                  r'\b(pytest|unittest|jest)\b',
                  r'test_.*\.py',
                  r'.*_test\.py',
                  r'.*\.spec\.(ts|js)',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "bugfix": {
              "name": "Bug Fixes",
              "patterns": [
                  r'^fix(\(|:|\s)',
                  r'^bugfix(\(|:|\s)',
                  r'^hotfix(\(|:|\s)',
                  r'^patch(\(|:|\s)',
                  r'\b(fix|fixes|fixed|fixing)\b',
                  r'\b(bug|bugs)\b',
                  r'\b(resolve|resolves|resolved|resolving)\b',
                  r'\b(issue|issues)\s*#?\d+',
                  r'\b(error|errors)\b',
                  r'\b(crash|crashes|crashing)\b',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "docs": {
              "name": "Documentation",
              "patterns": [
                  r'^docs?(\(|:|\s)',
                  r'\b(readme|README)\b',
                  r'\b(documentation|documenting)\b',
                  r'\b(docstring|docstrings)\b',
                  r'\b(comment|comments|commenting)\b',
                  r'\b(changelog|CHANGELOG)\b',
                  r'\.md$',
                  r'\.rst$',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "incident": {
              "name": "Incident Response",
              "patterns": [
                  r'\b(incident|incidents)\b',
                  r'\b(alert|alerts|alerting)\b',
                  r'\b(outage|outages)\b',
                  r'\b(emergency)\b',
                  r'\b(prod|production)\s+(fix|issue|bug|hotfix)',
                  r'\b(rollback|rollbacks|rolling back)\b',
                  r'\b(revert|reverting|reverted)\b',
                  r'\b(urgent|critical)\b',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "chore": {
              "name": "Chores/Maintenance",
              "patterns": [
                  r'^chore(\(|:|\s)',
                  r'^style(\(|:|\s)',
                  r'^build(\(|:|\s)',
                  r'\b(cleanup|clean up|cleaning)\b',
                  r'\b(lint|linting|linted)\b',
                  r'\b(format|formatting|formatted)\b',
                  r'\b(dependency|dependencies|deps)\b',
                  r'\b(upgrade|upgrading|upgraded)\b',
                  r'\b(bump|bumping|bumped)\b',
                  r'\b(merge|merging)\s+(branch|main|master)',
              ],
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
          "other": {
              "name": "Other",
              "patterns": [],  # Catch-all for unmatched commits
              "commits": 0,
              "lines_added": 0,
              "lines_removed": 0,
          },
      }
      
      result = categories
    output: categories

  # ==================== GIT DATA COLLECTION ====================

  - name: collect_git_commits
    description: "Collect commits from all repos using subprocess"
    compute: |
      import subprocess
      import re
      import os
      from copy import deepcopy
      
      # Initialize category stats
      cat_stats = deepcopy(categories)
      
      # Collect all commits with repo info
      all_commits = []
      repo_stats = {}
      
      def get_git_commits(repo_path, repo_name, author, since, until):
          """Get commits from a repo using git log directly."""
          commits = []
          
          if not repo_path or not os.path.isdir(repo_path):
              return commits, f"Path not found: {repo_path}"
          
          try:
              # Run git log with numstat
              cmd = [
                  "git", "log",
                  f"--author={author}",
                  f"--since={since}",
                  f"--until={until}",
                  "--pretty=format:COMMIT:%H|%s",
                  "--numstat",
                  "-n", "500"
              ]
              
              result = subprocess.run(
                  cmd,
                  cwd=repo_path,
                  capture_output=True,
                  text=True,
                  timeout=60
              )
              
              if result.returncode != 0:
                  return commits, f"Git error: {result.stderr}"
              
              # Parse output
              current_commit = None
              for line in result.stdout.split('\n'):
                  if line.startswith('COMMIT:'):
                      # Save previous commit
                      if current_commit:
                          commits.append(current_commit)
                      
                      # Parse new commit
                      parts = line[7:].split('|', 1)
                      sha = parts[0][:7] if parts else "unknown"
                      message = parts[1] if len(parts) > 1 else ""
                      
                      current_commit = {
                          "sha": sha,
                          "message": message[:200],
                          "repo": repo_name,
                          "lines_added": 0,
                          "lines_removed": 0,
                          "categories": [],
                      }
                  elif current_commit and line.strip():
                      # Numstat line: "123\t45\tfilename"
                      numstat_match = re.match(r'^(\d+|-)\t(\d+|-)\t', line)
                      if numstat_match:
                          added = numstat_match.group(1)
                          removed = numstat_match.group(2)
                          current_commit["lines_added"] += int(added) if added != '-' else 0
                          current_commit["lines_removed"] += int(removed) if removed != '-' else 0
              
              # Don't forget last commit
              if current_commit:
                  commits.append(current_commit)
              
              return commits, None
              
          except subprocess.TimeoutExpired:
              return commits, "Timeout"
          except Exception as e:
              return commits, str(e)
      
      # Collect from all repos and all authors
      repo_errors = []
      authors = ctx.get("authors", [])
      seen_commits = set()  # Track by SHA to avoid duplicates across authors
      
      for repo_info in ctx.get("repos", []):
          repo_name = repo_info.get("name", "unknown")
          repo_path = repo_info.get("path", "")
          
          # Collect commits for each author
          for author in authors:
              commits, error = get_git_commits(
                  repo_path,
                  repo_name,
                  author,
                  ctx.get("start_date", ""),
                  ctx.get("end_date", "")
              )
              
              if error:
                  repo_errors.append(f"{repo_name} ({author}): {error}")
                  continue
              
              # Deduplicate commits (same SHA across different author emails)
              for commit in commits:
                  commit_key = f"{repo_name}:{commit['sha']}"
                  if commit_key not in seen_commits:
                      seen_commits.add(commit_key)
                      all_commits.append(commit)
                      
                      # Initialize repo stats
                      if repo_name not in repo_stats:
                          repo_stats[repo_name] = {
                              "commits": 0,
                              "lines_added": 0,
                              "lines_removed": 0,
                              "primary_category": None,
                              "category_counts": {},
                          }
                      
                      # Update repo stats
                      repo_stats[repo_name]["commits"] += 1
                      repo_stats[repo_name]["lines_added"] += commit.get("lines_added", 0)
                      repo_stats[repo_name]["lines_removed"] += commit.get("lines_removed", 0)
      
      result = {
          "all_commits": all_commits,
          "repo_stats": repo_stats,
          "repo_errors": repo_errors,
          "categories": cat_stats,
      }
    output: git_data

  # ==================== CATEGORIZE COMMITS ====================

  - name: categorize_commits
    description: "Categorize commits by work type"
    compute: |
      import re
      
      all_commits = git_data.get("all_commits", [])
      repo_stats = git_data.get("repo_stats", {})
      cat_stats = git_data.get("categories", {})
      
      def categorize_commit(commit, cat_patterns):
          """Categorize a commit based on its message. Can match multiple categories."""
          message = commit.get("message", "").lower()
          matched_categories = []
          
          for cat_key, cat_info in cat_patterns.items():
              if cat_key == "other":
                  continue  # Handle "other" separately
              
              for pattern in cat_info.get("patterns", []):
                  if re.search(pattern, message, re.IGNORECASE):
                      matched_categories.append(cat_key)
                      break
          
          # If no category matched, mark as "other"
          if not matched_categories:
              matched_categories = ["other"]
          
          return matched_categories
      
      # Categorize each commit and update stats
      for commit in all_commits:
          repo_name = commit.get("repo", "unknown")
          
          # Ensure repo stats exist
          if repo_name not in repo_stats:
              repo_stats[repo_name] = {
                  "commits": 0,
                  "lines_added": 0,
                  "lines_removed": 0,
                  "primary_category": None,
                  "category_counts": {},
              }
          
          # Categorize
          matched_cats = categorize_commit(commit, cat_stats)
          commit["categories"] = matched_cats
          
          # Update category stats
          for cat in matched_cats:
              cat_stats[cat]["commits"] += 1
              cat_stats[cat]["lines_added"] += commit.get("lines_added", 0)
              cat_stats[cat]["lines_removed"] += commit.get("lines_removed", 0)
              
              # Track for repo stats
              if cat not in repo_stats[repo_name]["category_counts"]:
                  repo_stats[repo_name]["category_counts"][cat] = 0
              repo_stats[repo_name]["category_counts"][cat] += 1
      
      # Determine primary category for each repo
      for repo_name, stats in repo_stats.items():
          if stats.get("category_counts"):
              primary = max(stats["category_counts"].items(), key=lambda x: x[1])
              stats["primary_category"] = primary[0]
      
      # Calculate totals
      total_commits = len(all_commits)
      total_lines_added = sum(c.get("lines_added", 0) for c in all_commits)
      total_lines_removed = sum(c.get("lines_removed", 0) for c in all_commits)
      
      result = {
          "categories": cat_stats,
          "repo_stats": repo_stats,
          "commits": all_commits,
          "total_commits": total_commits,
          "total_lines_added": total_lines_added,
          "total_lines_removed": total_lines_removed,
          "repo_errors": git_data.get("repo_errors", []),
      }
    output: commit_analysis

  # ==================== JIRA DATA ====================

  - name: get_jira_completed
    description: "Get Jira issues completed in date range"
    tool: jira_search
    args:
      jql: "assignee = currentUser() AND (status changed to Done DURING ('{{ ctx.start_date }}', '{{ ctx.end_date }}') OR status changed to Closed DURING ('{{ ctx.start_date }}', '{{ ctx.end_date }}')) ORDER BY updated DESC"
      max_results: 50
    output: jira_completed_raw
    on_error: continue

  - name: get_jira_worked
    description: "Get Jira issues worked on in date range"
    tool: jira_search
    args:
      jql: "assignee = currentUser() AND updated >= '{{ ctx.start_date }}' AND updated <= '{{ ctx.end_date }}' ORDER BY updated DESC"
      max_results: 50
    output: jira_worked_raw
    on_error: continue

  - name: parse_jira_data
    description: "Parse Jira search results"
    compute: |
      import re
      from scripts.common.parsers import parse_jira_issues
      
      completed_issues = parse_jira_issues(str(jira_completed_raw) if jira_completed_raw else "")
      worked_issues = parse_jira_issues(str(jira_worked_raw) if jira_worked_raw else "")
      
      # Extract story points if available (look for patterns like "SP: 5" or "Story Points: 5")
      total_story_points = 0
      for issue in completed_issues:
          # Try to extract story points from summary or raw data
          raw_text = str(jira_completed_raw) if jira_completed_raw else ""
          sp_match = re.search(rf'{issue["key"]}.*?(\d+)\s*(?:SP|story\s*points?|points?)', raw_text, re.IGNORECASE)
          if sp_match:
              try:
                  total_story_points += int(sp_match.group(1))
              except ValueError:
                  pass
      
      result = {
          "completed": completed_issues,
          "completed_count": len(completed_issues),
          "worked": worked_issues,
          "worked_count": len(worked_issues),
          "story_points": total_story_points,
      }
    output: jira_data

  # ==================== GITLAB MR DATA ====================

  # Get MRs created by user
  - name: get_mrs_created
    description: "Get MRs created in date range"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      author: "@me"
      state: "all"
    output: mrs_created_raw
    on_error: continue

  # Get MRs reviewed by user
  - name: get_mrs_reviewed
    description: "Get MRs reviewed in date range"
    tool: gitlab_mr_list
    args:
      project: "automation-analytics/automation-analytics-backend"
      reviewer: "@me"
      state: "all"
    output: mrs_reviewed_raw
    on_error: continue

  - name: parse_gitlab_data
    description: "Parse GitLab MR data"
    compute: |
      from scripts.common.parsers import parse_mr_list
      from datetime import datetime
      
      start_date = datetime.strptime(ctx["start_date"], "%Y-%m-%d")
      end_date = datetime.strptime(ctx["end_date"], "%Y-%m-%d")
      
      # Parse MR lists
      created_mrs = parse_mr_list(str(mrs_created_raw) if mrs_created_raw else "")
      reviewed_mrs = parse_mr_list(str(mrs_reviewed_raw) if mrs_reviewed_raw else "")
      
      # Filter by date range (if dates available in MR data)
      # For now, just use the counts as-is since date filtering is complex
      
      # Count merged MRs from created list
      merged_count = sum(1 for mr in created_mrs if mr.get("status", "").lower() == "merged")
      
      result = {
          "created": created_mrs[:20],
          "created_count": len(created_mrs),
          "reviewed": reviewed_mrs[:20],
          "reviewed_count": len(reviewed_mrs),
          "merged_count": merged_count,
      }
    output: gitlab_data

  # ==================== GENERATE REPORT ====================

  - name: generate_report
    description: "Generate markdown report"
    compute: |
      lines = []
      
      # Header
      lines.append("# Work Analysis Report")
      lines.append("")
      lines.append(f"**Period:** {ctx['start_date']} to {ctx['end_date']}")
      lines.append(f"**Author:** {ctx['author_name']}")
      # Show author emails if multiple
      authors = ctx.get('authors', [])
      if len(authors) > 1:
          lines.append(f"**Accounts:** {', '.join(authors)}")
      elif len(authors) == 1:
          lines.append(f"**Email:** {authors[0]}")
      lines.append(f"**Repositories Analyzed:** {ctx['repo_count']}")
      lines.append("")
      lines.append("---")
      lines.append("")
      
      # Summary
      lines.append("## Summary")
      lines.append("")
      lines.append(f"- **Total Commits:** {commit_analysis['total_commits']}")
      lines.append(f"- **Lines Changed:** +{commit_analysis['total_lines_added']:,} / -{commit_analysis['total_lines_removed']:,}")
      lines.append(f"- **MRs Created:** {gitlab_data.get('created_count', 0)}")
      lines.append(f"- **MRs Reviewed:** {gitlab_data.get('reviewed_count', 0)}")
      lines.append(f"- **MRs Merged:** {gitlab_data.get('merged_count', 0)}")
      lines.append(f"- **Jira Issues Completed:** {jira_data.get('completed_count', 0)}")
      if jira_data.get('story_points', 0) > 0:
          lines.append(f"- **Story Points:** {jira_data['story_points']}")
      lines.append("")
      
      # Work Distribution
      lines.append("## Work Distribution")
      lines.append("")
      lines.append("| Category | Commits | Lines (+/-) | % of Work |")
      lines.append("|----------|---------|-------------|-----------|")
      
      # Sort categories by commit count
      cat_stats = commit_analysis.get("categories", {})
      total = commit_analysis.get("total_commits", 1) or 1
      
      sorted_cats = sorted(
          [(k, v) for k, v in cat_stats.items() if v.get("commits", 0) > 0],
          key=lambda x: x[1].get("commits", 0),
          reverse=True
      )
      
      for cat_key, cat_data in sorted_cats:
          name = cat_data.get("name", cat_key)
          commits = cat_data.get("commits", 0)
          added = cat_data.get("lines_added", 0)
          removed = cat_data.get("lines_removed", 0)
          pct = (commits / total) * 100 if total > 0 else 0
          lines.append(f"| {name} | {commits} | +{added:,}/-{removed:,} | {pct:.0f}% |")
      
      lines.append("")
      
      # By Repository
      lines.append("## By Repository")
      lines.append("")
      lines.append("| Repository | Commits | Lines (+/-) | Primary Category |")
      lines.append("|------------|---------|-------------|------------------|")
      
      repo_stats = commit_analysis.get("repo_stats", {})
      for repo_name, stats in sorted(repo_stats.items(), key=lambda x: x[1].get("commits", 0), reverse=True):
          commits = stats.get("commits", 0)
          added = stats.get("lines_added", 0)
          removed = stats.get("lines_removed", 0)
          primary = stats.get("primary_category", "other")
          primary_name = cat_stats.get(primary, {}).get("name", primary)
          lines.append(f"| {repo_name} | {commits} | +{added:,}/-{removed:,} | {primary_name} |")
      
      lines.append("")
      
      # Jira Activity
      if jira_data.get("completed"):
          lines.append("## Jira Issues Completed")
          lines.append("")
          for issue in jira_data["completed"][:10]:
              key = issue.get("key", "?")
              summary = issue.get("summary", "")[:60]
              lines.append(f"- [{key}](https://issues.redhat.com/browse/{key}): {summary}")
          if len(jira_data["completed"]) > 10:
              lines.append(f"- ... and {len(jira_data['completed']) - 10} more")
          lines.append("")
      
      # MR Activity
      if gitlab_data.get("created"):
          lines.append("## MRs Created")
          lines.append("")
          for mr in gitlab_data["created"][:5]:
              mr_id = mr.get("id", mr.get("iid", "?"))
              title = mr.get("title", "")[:50]
              status = mr.get("status", "open")
              lines.append(f"- !{mr_id}: {title} ({status})")
          lines.append("")
      
      if gitlab_data.get("reviewed"):
          lines.append("## MRs Reviewed")
          lines.append("")
          for mr in gitlab_data["reviewed"][:5]:
              mr_id = mr.get("id", mr.get("iid", "?"))
              title = mr.get("title", "")[:50]
              lines.append(f"- !{mr_id}: {title}")
          lines.append("")
      
      # Recent Commits Sample
      if commit_analysis.get("commits"):
          lines.append("## Recent Commits (Sample)")
          lines.append("")
          for commit in commit_analysis["commits"][:10]:
              sha = commit.get("sha", "?")[:7]
              msg = commit.get("message", "")[:60]
              repo = commit.get("repo", "")
              cats = ", ".join(commit.get("categories", ["other"]))
              lines.append(f"- `{sha}` [{repo}] {msg} *({cats})*")
          lines.append("")
      
      # Footer
      lines.append("---")
      lines.append("")
      lines.append(f"*Generated by work_analysis skill on {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M')}*")
      
      result = "\n".join(lines)
    output: report

  # ==================== MEMORY ====================

  - name: log_analysis_session
    description: "Log work analysis to session"
    tool: memory_session_log
    args:
      action: "Generated work analysis report"
      details: "Period: {{ ctx.start_date }} to {{ ctx.end_date }}, Commits: {{ commit_analysis.total_commits }}, Repos: {{ ctx.repo_count }}"
    on_error: continue

  - name: save_analysis_to_memory
    description: "Save analysis summary to memory for future reference"
    compute: |
      from datetime import datetime
      
      # Load patterns
      patterns = memory.read_memory("learned/patterns") or {}
      if "work_analyses" not in patterns:
          patterns["work_analyses"] = []
      
      # Record this analysis
      analysis_record = {
          "start_date": ctx["start_date"],
          "end_date": ctx["end_date"],
          "total_commits": commit_analysis.get("total_commits", 0),
          "total_lines_added": commit_analysis.get("total_lines_added", 0),
          "total_lines_removed": commit_analysis.get("total_lines_removed", 0),
          "repos_analyzed": ctx.get("repo_count", 0),
          "jira_completed": jira_data.get("completed_count", 0),
          "mrs_created": gitlab_data.get("created_count", 0),
          "mrs_reviewed": gitlab_data.get("reviewed_count", 0),
          "timestamp": datetime.now().isoformat(),
      }
      
      patterns["work_analyses"].append(analysis_record)
      
      # Keep last 20 analyses
      patterns["work_analyses"] = patterns["work_analyses"][-20:]
      
      memory.write_memory("learned/patterns", patterns)
      result = "analysis saved to memory"
    output: memory_save_result
    on_error: continue

# ==================== OUTPUT ====================

outputs:
  - name: summary
    value: "{{ report }}"

  - name: metrics
    value:
      period: "{{ ctx.start_date }} to {{ ctx.end_date }}"
      total_commits: "{{ commit_analysis.total_commits }}"
      total_lines_added: "{{ commit_analysis.total_lines_added }}"
      total_lines_removed: "{{ commit_analysis.total_lines_removed }}"
      repos_analyzed: "{{ ctx.repo_count }}"
      jira_completed: "{{ jira_data.completed_count }}"
      mrs_created: "{{ gitlab_data.created_count }}"
      mrs_reviewed: "{{ gitlab_data.reviewed_count }}"
