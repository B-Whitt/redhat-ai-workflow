metadata:
  project: dataverse-integration
  persona: developer
  last_updated: '2026-02-02T20:27:10.628955'
  last_scanned: '2026-02-02T18:54:36.098294'
  confidence: 0.9500000000000003
  auto_generated: true
architecture:
  overview: 'Documentation/research repository for Ansible Analytics migration from
    RedShift to Dataverse/Snowflake. Contains:

    - Migration planning docs (docs/)

    - Schema reference CSVs (schema/) - 13 Ansible data dictionary tables

    - Meeting notes and entries (entries/YYYY/MM/DD/)

    - CLI skill references (gcmd, jirahhh, slacker)


    NO CODE - pure documentation. No build/test/lint commands.'
  key_modules: []
  data_flow: ''
  dependencies: []
patterns:
  coding: []
  testing: []
  deployment: []
gotchas: []
learned_from_tasks: []
business_context:
  URGENT: Tableau/TeleSense reports paused since Q4 2025 due to RedShift discontinuation.
  Root causes:
  - 85% drop in telemetry due to service account configuration burden
  - RedShift infrastructure end-of-life
  - Schema incompatibility with Dataverse
  Solution: Certificate-based authentication (ANSTRAT-1858) eliminating customer friction.
  Timeline: 8-week restoration plan targeting Q1-Q2 2026.
key_jira_issues:
  Strategic:
  - ANSTRAT-1500: Dataverse export initiative
  - ANSTRAT-1858: Certificate authentication (breakthrough)
  Implementation:
  - AAP-62936: Certificate auth architecture
  - AAP-47464: Dataverse export implementation
  - AAP-63982: Architecture investigation spike
  - AAP-63841: dbt/Snowpipe skills development (David O'Neill)
stakeholders:
- Ben Thomasson (@bthomass) - Project lead, ANSTRAT owner
- Priya Narayan (@pnarayan) - Product manager
- Aparna Karve (@akarve) - Implementation lead
- David O'Neill (@doneill) - Technical implementation
- Sadaf Shaikh, Heiko Rupp - Analytics team
- Kipi Team - External dataverse consulting
tech_stack:
  Current (Legacy):
  - "Ansible Metrics \u2192 Red Hat Insights \u2192 S3 \u2192 RedShift \u2192 Tableau"
  - Airflow-managed ETL
  Target:
  - "AAP Metrics \u2192 Certificate Auth \u2192 S3 \u2192 Snowpipe \u2192 Snowflake/Dataverse\
    \ \u2192 dbt \u2192 Tableau"
  - ELT architecture with Snowflake Tasks & Streams
  - dbt for transformations
tracking_issue: AAP-63841 - dbt/Snowpipe learning initiative and dataverse migration
  skills development (David O'Neill)
dbt_setup: 'Local dbt environment setup:


  1. mkdir -vp ~/.dbt

  2. pipenv --python 3.12

  3. pipenv install dbt-core dbt-snowflake

  4. pipenv shell && dbt init


  Virtual env: ~/.local/share/virtualenvs/dataverse-integration-LWALY2Zy


  Snowflake connection (PENDING VERIFICATION):

  - ACCOUNT: GDADCLC-RHPROD

  - USER: DAONEILL

  - ROLE: CIAM_MARTS_GROUP (may need Ansible-specific role)

  - WAREHOUSE: DEFAULT

  - DATABASE: CIAM_DB (may need Ansible-specific DB)

  - SCHEMA: MARTS (expected: snowpipe_db for Ansible)


  See: docs/dbt-setup.md'
access_requirements: "Dataverse/Snowflake Access Requirements:\n\nEnvironments needed:\n\
  - dev: Local development & testing\n- stage: Pre-production validation with Tableau\n\
  - prod: Production data\n\nPermissions needed:\n- Schema creation (13 tables from\
  \ RedShift)\n- Snowpipe configuration (S3 \u2192 Snowflake)\n- dbt execution permissions\n\
  \nHow to request:\n1. Slack: #help-dataverse-platform\n2. Data product config system\n\
  3. Reference: OpenShift team pattern (Martin Bacovsky)\n\nTables to migrate: ansible_cluster_info,\
  \ ansible_cluster_meta, ansible_host_info, ansible_job_info, ansible_events_data,\
  \ ansible_entitlements_data, ansible_daily_host_count, ansible_host_metrics, ansible_job_rollup,\
  \ ansible_modules_rules, ansible_lightspeed_product_feedback, ansible_lightspeed_recommendation,\
  \ ansible_one_click_trail"
platform_concepts: "Snowflake vs Dataverse:\n\n- Dataverse = Red Hat's enterprise\
  \ data platform (wrapper around Snowflake)\n- Snowflake = The underlying cloud data\
  \ warehouse engine\n- Snowpipe = Snowflake's data ingestion (S3 \u2192 Snowflake)\n\
  - dbt = Transformation tool running inside Snowflake\n- Data Product = Your configured\
  \ dataset in Dataverse\n\nWhen you 'request Snowflake access', you're requesting\
  \ a Dataverse data product.\n\nSupport: #help-dataverse-platform (Slack)\nExamples:\
  \ OpenShift team (Martin Bacovsky)\n\nRed Hat enterprise-wide consolidation to Dataverse\
  \ by end of CY25."
dataverse_resources: 'Official Dataverse Documentation:

  - Landing Page: https://dataverse.pages.redhat.com/

  - Getting Started: https://dataverse.pages.redhat.com/getting-started/

  - TOC: https://dataverse.pages.redhat.com/community/toc/


  Slack Channels:

  - #help-dataverse-platform - User support (Mon-Fri working hours)

  - #forum-dataverse - Community discussions

  - @dataverse-platform-oncall-primary - MR discussions & support

  - @dataverse-platform-oncall-secondary - Backup oncall


  Forms:

  - Data Product Access Request Form

  - Atlan Access Request Form

  - Dataverse User Support Form (for issues)

  - Review my MR Form (for MR reviews)


  Support: Mon-Fri, weekly shifts'
data_product_status: 'Data Product: aapautomationanalytics

  Status: EXISTS but NO SCHEMAS defined

  Issue: ''No presentation schemas available for this product''


  Next Steps:

  1. Find dataproduct-config repo for aapautomationanalytics

  2. Create schema definitions for 13 tables

  3. Submit MR and use ''Review my MR'' form

  4. Or request help via User Support Form in #help-dataverse-platform


  Users to add:

  - daoneill@redhat.com

  - zsadeh@redhat.com (Manager)

  - bthomass@redhat.com (Project lead)


  Need to create schemas BEFORE requesting consumer access.'
data_product_setup:
  Data Product Owner Setup Process: null
  1. Role: Source Product Owner (not Consumer) - Consumer = read existing data - Source
    Product Owner = create & manage data products
  2. Setup Steps: a. Follow Source Product Owner guide at dataverse.pages.redhat.com/getting-started/
    b. Create GitLab repo (aapautomationanalytics-dbt) c. Create dbt project with
    models for 13 tables d. Define MARTS schema as Output Ports e. Configure Snowpipe
    for S3 ingestion
  3. Unlock Discovery (after prod deploy):
  - Add Readme Template to dataproduct-dbt folder
  - Assign Owners in Atlan
  - Designate Output Ports (tables/views in MARTS)
  - Notify
  Note: Atlan only shows production-ready assets.
gitlab_repo: 'AAP Automation Analytics dbt Repo:


  URL: https://gitlab.cee.redhat.com/dataverse/data-products/source-aligned/aapautomationanalytics/aapautomationanalytics-dbt


  Status: EXISTS but EMPTY (placeholder models)

  - README.md: Default GitLab template

  - models/source.yml: Empty (sources: [])

  - models/staging/staging.sql: Placeholder (select current_timestamp)

  - models/marts/marts.sql: Placeholder (select current_timestamp)


  Why ''No presentation schemas'': Marts are empty placeholders


  Next steps:

  1. Clone repo

  2. Add real models for 13 tables

  3. Update source.yml with Snowpipe sources

  4. Create staging + marts models

  5. Update README

  6. Submit MR'
