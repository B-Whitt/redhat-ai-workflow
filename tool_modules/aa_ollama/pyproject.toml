[project]
name = "aa-ollama"
version = "0.1.0"
description = "AA MCP Server - Local Ollama inference tools for NPU/GPU/CPU"
requires-python = ">=3.11"
dependencies = [
    "mcp>=1.0.0",
    "httpx>=0.27.0",
    "pyyaml>=6.0",
]

[project.scripts]
aa-ollama = "aa_ollama.server:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]
