# Google Meet Bot - Technical Plan (Option C: Hybrid)

## Overview

A bot that joins Google Meetings from your Google Calendar, provides real-time transcription, responds to wake-word "David", and uses AI to handle Jira/project queries with voice and video avatar responses.

**Hardware Constraints:**
- RTX 4060 (6GB VRAM) - Primary GPU
- Intel NPU (OpenVINO) - STT/Wake-word
- Intel iGPU - LLM inference
- CPU - Fallback/orchestration

---

## Implementation Status

### Phase 1: Foundation (COMPLETED)

| Component | Status | Location |
|-----------|--------|----------|
| Module structure | âœ… Done | `tool_modules/aa_meet_bot/` |
| Configuration | âœ… Done | `config.json` â†’ `meet_bot` section |
| Virtual audio (PulseAudio) | âœ… Done | `src/virtual_devices.py` |
| Virtual video (v4l2loopback) | âœ… Done | `src/virtual_devices.py` |
| Browser controller | âœ… Done | `src/browser_controller.py` |
| Caption capture | âœ… Done | `src/browser_controller.py` |
| Wake word detection | âœ… Done | `src/wake_word.py` |
| MCP tools | âœ… Done | `src/tools_basic.py` |

### Phase 2: Voice/Video (COMPLETED)

| Component | Status | Location |
|-----------|--------|----------|
| Voice samples | âœ… Done | `phonetics.wav` (7:19) |
| GPT-SoVITS model | âœ… Done | `~/src/GPT-SoVITS/GPT_weights_v2Pro/dave-e50.ckpt` |
| TTS engine | âœ… Done | `src/tts_engine.py` |
| Wav2Lip integration | âœ… Done | `src/video_generator.py` |
| Video fallback | âœ… Done | Static image + audio via ffmpeg |
| MCP tools (TTS/Video) | âœ… Done | `meet_bot_synthesize_speech`, `meet_bot_generate_video`, `meet_bot_respond` |

### Phase 3: Integration (COMPLETED)

| Component | Status | Location |
|-----------|--------|----------|
| LLM response pipeline | âœ… Done | `src/llm_responder.py` |
| Jira context preloading | âœ… Done | `src/jira_preloader.py` |
| Command Center UI | âœ… Done | `extensions/aa_workflow_vscode/src/meetingsTab.ts` |

### Phase 4: Testing (COMPLETED)

| Component | Status | Notes |
|-----------|--------|-------|
| End-to-end test | âœ… Done | `scripts/test_e2e.py` |
| Wav2Lip setup | âœ… Done | Model downloaded, real-time lip-sync working |
| Pre-generated clips | âŒ Cancelled | Wav2Lip real-time works well |

### Phase 5: Integration Testing (COMPLETED)

| Component | Status | Notes |
|-----------|--------|-------|
| VPN + Jira | âœ… Done | 50+ issues loaded via REST API |
| VS Code Extension | âœ… Done | `aa-workflow-0.1.0.vsix` installed |
| Meetings Tab | âœ… Done | Video preview, transcription, controls |

## Test Results (Latest: 2026-01-18)

```
âœ… Configuration: PASS
âœ… Virtual Devices: PASS (PulseAudio sink/source + /dev/video10)
âœ… Wake Word Detection: PASS (text-based regex on captions)
âœ… LLM Response: PASS (Ollama qwen2.5:0.5b on iGPU)
âœ… TTS Synthesis: PASS (GPT-SoVITS voice cloning)
âœ… Video Generation: PASS (Wav2Lip real-time @ 600x600)
âœ… Jira Preload: PASS (50+ issues loaded via REST API)
âœ… VS Code Extension: PASS (Meetings tab installed)
```

### Video Generation Details
- **Resolution**: 600Ã—600 (upscaled from Wav2Lip output)
- **Duration**: Matches audio length
- **Model**: wav2lip_gan.pth
- **VRAM Usage**: ~3-4GB during generation

---

## Quick Start

```bash
# 1. Set up virtual devices
meet_bot_setup_devices()

# 2. Check status
meet_bot_status()

# 3. Test avatar image
meet_bot_test_avatar()

# 4. Approve a meeting
meet_bot_approve_meeting("https://meet.google.com/xxx-xxxx-xxx", "Test Meeting")

# 5. Join the meeting
meet_bot_join_meeting()
```

---

## Architecture: Option C - Hybrid Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           GOOGLE MEET BOT ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Google     â”‚    â”‚   Command    â”‚    â”‚   Meeting    â”‚                   â”‚
â”‚  â”‚   Calendar   â”‚â”€â”€â”€â–¶â”‚   Center     â”‚â”€â”€â”€â–¶â”‚   Approval   â”‚                   â”‚
â”‚  â”‚   Monitor    â”‚    â”‚   UI (VSCode)â”‚    â”‚   Queue      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                              â”‚                   â”‚                           â”‚
â”‚                              â–¼                   â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    MEETING JOIN (Browser Automation)                 â”‚    â”‚
â”‚  â”‚  Puppeteer/Playwright â†’ Google Meet URL â†’ Virtual Camera/Mic        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â–¼                    â–¼                    â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Audio      â”‚    â”‚   Wake-Word  â”‚    â”‚   Jira       â”‚                   â”‚
â”‚  â”‚   Capture    â”‚    â”‚   Detection  â”‚    â”‚   Preloader  â”‚                   â”‚
â”‚  â”‚  (PulseAudio)â”‚    â”‚  (Porcupine) â”‚    â”‚  (Sprint)    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                    â”‚                    â”‚                         â”‚
â”‚         â–¼                    â–¼                    â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚                         â”‚
â”‚  â”‚   Whisper    â”‚    â”‚   Voice      â”‚            â”‚                         â”‚
â”‚  â”‚   STT (NPU)  â”‚â—€â”€â”€â”€â”‚   Activity   â”‚            â”‚                         â”‚
â”‚  â”‚   OpenVINO   â”‚    â”‚   Detection  â”‚            â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                         â”‚
â”‚         â”‚                                         â”‚                         â”‚
â”‚         â–¼                                         â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         LLM PROCESSOR (iGPU)                         â”‚    â”‚
â”‚  â”‚  Context: Transcription + Jira Sprint Data + Wake-word Trigger      â”‚    â”‚
â”‚  â”‚  Output: Natural language response (no code, short status updates)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â–¼                                         â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   PHRASE MATCHER     â”‚              â”‚   REAL-TIME PATH     â”‚             â”‚
â”‚  â”‚   (Pre-gen Library)  â”‚              â”‚   (Novel Responses)  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                                         â”‚                         â”‚
â”‚         â–¼                                         â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Pre-Generated      â”‚              â”‚   GPT-SoVITS TTS     â”‚             â”‚
â”‚  â”‚   Video Clips        â”‚              â”‚   (~2GB VRAM)        â”‚             â”‚
â”‚  â”‚   (512Ã—512, HQ)      â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚                          â”‚
â”‚         â”‚                                         â–¼                         â”‚
â”‚         â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚                              â”‚   Wav2Lip/FlashLips  â”‚             â”‚
â”‚         â”‚                              â”‚   256Ã—256 @ 25 FPS   â”‚             â”‚
â”‚         â”‚                              â”‚   (~3-4GB VRAM)      â”‚             â”‚
â”‚         â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                                         â”‚                         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    OUTPUT SYNCHRONIZATION                            â”‚    â”‚
â”‚  â”‚  Audio: PulseAudio Virtual Sink â†’ Google Meet Microphone            â”‚    â”‚
â”‚  â”‚  Video: v4l2loopback Virtual Camera â†’ Google Meet Camera            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hardware Allocation

### Caption-First Mode (Primary - Minimal Resources)

| Component | Hardware | VRAM/Memory | Power | Latency Target |
|-----------|----------|-------------|-------|----------------|
| Caption Capture | CPU | ~10MB | <1W | ~1-2s (Google's latency) |
| Wake-word (text regex) | CPU | ~1MB | <1W | <1ms |
| LLM (qwen2.5:7b) | iGPU | ~8GB shared | 15-25W | <2s |
| GPT-SoVITS TTS | RTX 4060 | ~2GB | 40W | <1s |
| Wav2Lip/FlashLips | RTX 4060 | ~3-4GB | 60W | <100ms/frame |
| **Total GPU** | RTX 4060 | **~5-6GB** | 115W | - |

**NPU is FREE** in caption-first mode - available for other tasks!

### Whisper Fallback Mode (When captions unavailable)

| Component | Hardware | VRAM/Memory | Power | Latency Target |
|-----------|----------|-------------|-------|----------------|
| Whisper STT | NPU (OpenVINO) | ~500MB | 2-5W | <500ms |
| Wake-word (Porcupine) | NPU/CPU | ~50MB | 2-5W | <50ms |
| LLM (qwen2.5:7b) | iGPU | ~8GB shared | 15-25W | <2s |
| GPT-SoVITS TTS | RTX 4060 | ~2GB | 40W | <1s |
| Wav2Lip/FlashLips | RTX 4060 | ~3-4GB | 60W | <100ms/frame |
| **Total GPU** | RTX 4060 | **~5-6GB** | 115W | - |

---

## Component Details

### 1. Meeting Discovery & Approval

**Source:** Google Calendar API (existing `aa_google_calendar` module)

```yaml
# config.json addition
meet_bot:
  enabled: false
  wake_word: "david"
  approval_mode: "command_center"  # or "auto" for whitelisted organizers
  auto_approve_organizers:
    - "daoneill@redhat.com"
    - "bthomass@redhat.com"
  join_buffer_minutes: 2  # Join 2 minutes before start
```

**Workflow:**
1. Poll calendar every 5 minutes for upcoming meetings
2. Extract Google Meet URL from event
3. Add to approval queue in Command Center
4. On approval, schedule join at `start_time - buffer`

### 2. Meeting Join (Browser Automation)

**Primary:** Puppeteer with Chrome/Chromium
**Fallback:** Playwright

```python
# Puppeteer approach (Node.js subprocess or pyppeteer)
async def join_meeting(meet_url: str):
    browser = await launch(
        headless=False,  # Need visible for virtual camera
        args=[
            '--use-fake-ui-for-media-stream',
            '--use-fake-device-for-media-stream',
            f'--use-file-for-fake-video-capture={VIRTUAL_CAMERA}',
            f'--use-file-for-fake-audio-capture={VIRTUAL_MIC}',
        ]
    )
    page = await browser.newPage()
    await page.goto(meet_url)
    # Handle "Join now" button, permissions, etc.
```

**Challenges:**
- Google Meet detects automation â†’ Use undetected-chromedriver
- Need to handle "Ask to join" flow
- Must maintain session cookies for Google auth

### 3. Transcription Pipeline (Caption-First Strategy)

**Key Insight:** Google Meet already does STT for captions - we can capture that instead of running our own Whisper, saving significant NPU/GPU resources.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSCRIPTION PIPELINE (CAPTION-FIRST)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                     PRIMARY: CAPTION CAPTURE                     â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Google Meet UI â”€â”€â–¶ Enable Captions (CC button)                 â”‚    â”‚
â”‚  â”‚         â”‚                                                        â”‚    â”‚
â”‚  â”‚         â–¼                                                        â”‚    â”‚
â”‚  â”‚  Caption DOM â”€â”€â–¶ MutationObserver â”€â”€â–¶ Text Buffer               â”‚    â”‚
â”‚  â”‚  (.a4cQT class)      (watch for new caption elements)           â”‚    â”‚
â”‚  â”‚         â”‚                                                        â”‚    â”‚
â”‚  â”‚         â–¼                                                        â”‚    â”‚
â”‚  â”‚  Parse Speaker + Text â”€â”€â–¶ Transcription Stream                  â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  âœ… Zero local STT processing                                   â”‚    â”‚
â”‚  â”‚  âœ… Google's high-quality transcription                         â”‚    â”‚
â”‚  â”‚  âœ… Speaker identification included                             â”‚    â”‚
â”‚  â”‚  âš ï¸  ~1-2 second latency                                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                           â”‚
â”‚                              â–¼                                           â”‚
â”‚                    Caption Available?                                    â”‚
â”‚                     /            \                                       â”‚
â”‚                   YES             NO                                     â”‚
â”‚                    â”‚               â”‚                                     â”‚
â”‚                    â–¼               â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Use Caption Text    â”‚  â”‚  FALLBACK: LOCAL WHISPER (NPU)       â”‚    â”‚
â”‚  â”‚  (no processing)     â”‚  â”‚                                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Meeting Audio â”€â”€â–¶ PulseAudio Monitorâ”‚    â”‚
â”‚                             â”‚         â”‚                            â”‚    â”‚
â”‚                             â”‚         â–¼                            â”‚    â”‚
â”‚                             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚    â”‚
â”‚                             â”‚  â”‚   Whisper    â”‚                    â”‚    â”‚
â”‚                             â”‚  â”‚   STT (NPU)  â”‚                    â”‚    â”‚
â”‚                             â”‚  â”‚   OpenVINO   â”‚                    â”‚    â”‚
â”‚                             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                           â”‚                              â”‚
â”‚                                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      WAKE-WORD DETECTION                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Transcription Stream â”€â”€â–¶ Text Pattern Match "David"            â”‚    â”‚
â”‚  â”‚         â”‚                    (case-insensitive regex)           â”‚    â”‚
â”‚  â”‚         â–¼                                                        â”‚    â”‚
â”‚  â”‚  Wake Detected â”€â”€â–¶ Start Context Accumulation                   â”‚    â”‚
â”‚  â”‚         â”‚                                                        â”‚    â”‚
â”‚  â”‚         â–¼                                                        â”‚    â”‚
â”‚  â”‚  Pause Detection (no new text for 2-3 seconds)                  â”‚    â”‚
â”‚  â”‚         â”‚                                                        â”‚    â”‚
â”‚  â”‚         â–¼                                                        â”‚    â”‚
â”‚  â”‚  Send Context to LLM                                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Caption DOM Capture Implementation

Google Meet renders captions in a specific DOM structure. We can capture them with a MutationObserver:

```javascript
// Injected into Google Meet page via Puppeteer/content script
class CaptionCapture {
  constructor(onCaption) {
    this.onCaption = onCaption;
    this.lastText = '';
    this.observer = null;
  }

  start() {
    // Google Meet caption container selector (may change - needs monitoring)
    const captionSelectors = [
      '.a4cQT',           // Main caption container
      '[data-message-text]', // Alternative selector
      '.iTTPOb'           // Caption text spans
    ];

    this.observer = new MutationObserver((mutations) => {
      for (const mutation of mutations) {
        if (mutation.type === 'childList' || mutation.type === 'characterData') {
          this.extractCaption();
        }
      }
    });

    // Observe the entire body for caption changes
    this.observer.observe(document.body, {
      childList: true,
      subtree: true,
      characterData: true
    });

    console.log('[CaptionCapture] Started monitoring captions');
  }

  extractCaption() {
    // Try multiple selectors (Google changes these periodically)
    const captionEl = document.querySelector('.a4cQT') ||
                      document.querySelector('[data-message-text]');

    if (captionEl) {
      const speakerEl = captionEl.querySelector('.zs7s8d'); // Speaker name
      const textEl = captionEl.querySelector('.iTTPOb');    // Caption text

      const speaker = speakerEl?.textContent?.trim() || 'Unknown';
      const text = textEl?.textContent?.trim() || '';

      if (text && text !== this.lastText) {
        this.lastText = text;
        this.onCaption({ speaker, text, timestamp: Date.now() });
      }
    }
  }

  stop() {
    if (this.observer) {
      this.observer.disconnect();
    }
  }
}

// Usage in Puppeteer
await page.evaluate(() => {
  window.captionCapture = new CaptionCapture((caption) => {
    // Send to extension via postMessage
    window.postMessage({ type: 'CAPTION', ...caption }, '*');
  });
  window.captionCapture.start();
});
```

#### Auto-Enable Captions

```python
async def enable_captions(page):
    """Click the CC button to enable captions if not already on"""
    try:
        # Find and click the captions button
        cc_button = await page.querySelector('[aria-label="Turn on captions"]')
        if cc_button:
            await cc_button.click()
            print("[MeetBot] Captions enabled")
            return True
        else:
            # Check if already enabled
            cc_on = await page.querySelector('[aria-label="Turn off captions"]')
            if cc_on:
                print("[MeetBot] Captions already enabled")
                return True
    except Exception as e:
        print(f"[MeetBot] Failed to enable captions: {e}")
    return False
```

#### Fallback Detection Logic

```python
class TranscriptionManager:
    def __init__(self):
        self.caption_active = False
        self.last_caption_time = 0
        self.fallback_threshold_ms = 5000  # 5 seconds without captions
        self.whisper_model = None  # Lazy load

    async def on_caption(self, caption: dict):
        """Called when caption is captured from DOM"""
        self.caption_active = True
        self.last_caption_time = time.time() * 1000
        await self.process_transcription(caption['text'], caption['speaker'], source='caption')

    async def check_fallback(self):
        """Check if we need to fall back to local STT"""
        now = time.time() * 1000
        if self.caption_active and (now - self.last_caption_time) > self.fallback_threshold_ms:
            print("[TranscriptionManager] Captions stopped, switching to Whisper fallback")
            self.caption_active = False
            await self.start_whisper_fallback()

    async def start_whisper_fallback(self):
        """Initialize and start local Whisper STT on NPU"""
        if self.whisper_model is None:
            from optimum.intel import OVModelForSpeechSeq2Seq
            self.whisper_model = OVModelForSpeechSeq2Seq.from_pretrained(
                "openai/whisper-small",
                export=True,
                device="NPU"
            )
        # Start audio capture and processing...
```

#### Resource Comparison

| Mode | NPU Usage | GPU Usage | Latency | Quality |
|------|-----------|-----------|---------|---------|
| **Caption Capture** | 0% | 0% | ~1-2s | Excellent (Google's STT) |
| **Whisper Fallback** | ~80% | 0% | ~500ms | Very Good |

**When to use fallback:**
- Captions disabled by meeting host
- Caption DOM structure changed (Google update)
- Network issues causing caption lag
- Need lower latency for specific interactions

### 4. Wake-Word Detection

Since we're using caption text (not raw audio), wake-word detection becomes **text pattern matching** - much simpler!

#### Primary: Text-Based Detection (from captions)

```python
import re

class WakeWordDetector:
    def __init__(self, wake_word: str = "david"):
        self.wake_word = wake_word.lower()
        # Match "David" at word boundaries, case-insensitive
        self.pattern = re.compile(
            rf'\b{re.escape(self.wake_word)}\b',
            re.IGNORECASE
        )
        self.context_buffer = []
        self.listening = False
        self.last_text_time = 0
        self.pause_threshold_ms = 2500  # 2.5 seconds of silence = end of query

    def process_caption(self, text: str, timestamp: int) -> dict | None:
        """Process caption text, return context when query complete"""

        # Check for wake word
        if self.pattern.search(text):
            self.listening = True
            self.context_buffer = []
            # Extract text after wake word
            parts = self.pattern.split(text, maxsplit=1)
            if len(parts) > 1 and parts[1].strip():
                self.context_buffer.append(parts[1].strip())
            self.last_text_time = timestamp
            return None

        # If listening, accumulate context
        if self.listening:
            self.context_buffer.append(text)
            self.last_text_time = timestamp
            return None

        return None

    def check_pause(self, current_time: int) -> str | None:
        """Check if speaker has paused (query complete)"""
        if self.listening and self.context_buffer:
            if (current_time - self.last_text_time) > self.pause_threshold_ms:
                context = ' '.join(self.context_buffer)
                self.listening = False
                self.context_buffer = []
                return context
        return None

# Usage
detector = WakeWordDetector("david")

# From caption stream
for caption in caption_stream:
    detector.process_caption(caption['text'], caption['timestamp'])

    # Check for pause every 500ms
    query = detector.check_pause(time.time() * 1000)
    if query:
        # Send to LLM
        response = await llm.generate(query, jira_context)
```

#### Fallback: Audio-Based Detection (when captions unavailable)

If captions fail and we're using Whisper fallback, we can still do text-based detection on the Whisper output. However, for lower latency, we could use:

**Option A: Porcupine (Picovoice)** - Proprietary but excellent
- Custom wake-word "David" training via Picovoice Console
- <50ms latency on CPU
- Free tier: 3 custom wake words

**Option B: openWakeWord** - Fully open source
- Train custom model with ~50-100 samples
- Slightly higher latency (~100ms)
- No licensing restrictions

```python
# Only used when caption capture fails
import pvporcupine

class AudioWakeWordFallback:
    def __init__(self, keyword_path: str):
        self.porcupine = pvporcupine.create(
            keyword_paths=[keyword_path],
            sensitivities=[0.7]
        )

    def process_audio(self, pcm_frame) -> bool:
        """Returns True if wake word detected"""
        keyword_index = self.porcupine.process(pcm_frame)
        return keyword_index >= 0
```

#### Detection Flow

```
Caption Text â”€â”€â–¶ Regex Match "David" â”€â”€â–¶ Start Listening
                                              â”‚
                                              â–¼
                                    Accumulate Context
                                              â”‚
                                              â–¼
                              2.5s Pause? â”€â”€â”€YESâ”€â”€â–¶ Send to LLM
                                    â”‚
                                   NO
                                    â”‚
                                    â–¼
                              Continue Listening
```

### 5. Jira Context Preloader

Before joining meeting, preload:
```python
async def preload_jira_context():
    return {
        "sprint_issues": await jira_search(
            "project = AAP AND sprint in openSprints() AND assignee = currentUser()"
        ),
        "blocked_items": await jira_list_blocked(),
        "recent_comments": await get_recent_comments(days=3),
        "my_mrs": await gitlab_mr_list(author="@me", state="opened"),
    }
```

### 6. LLM Response Generation

**Model:** qwen2.5:7b on iGPU (Ollama)
**Constraints:**
- Natural language only (no code)
- Short responses (<30 words typical)
- Status update focused

```python
SYSTEM_PROMPT = """You are David's meeting assistant. You have access to his Jira
sprint data and can provide brief status updates.

RULES:
- Respond in natural conversational English
- Keep responses under 30 words
- Never output code or technical syntax
- Focus on status, blockers, and next steps
- Be concise - this is a live meeting

CONTEXT:
{jira_context}
"""
```

### 7. Hybrid Video Output System

#### Pre-Generated Clip Library

```yaml
# ~/.config/meet-bot/clips/library.yaml
clips:
  greetings:
    hello:
      path: "greetings/hello.mp4"
      text: "Hello everyone"
      duration_ms: 1200
      emotion: neutral
    good_morning:
      path: "greetings/good_morning.mp4"
      text: "Good morning"
      duration_ms: 1000
      emotion: cheerful

  transitions:
    let_me_check:
      path: "transitions/let_me_check.mp4"
      text: "Let me check on that"
      duration_ms: 1500
      emotion: neutral
    one_moment:
      path: "transitions/one_moment.mp4"
      text: "One moment please"
      duration_ms: 1200
      emotion: neutral

  status_templates:
    in_progress:
      path: "status/in_progress.mp4"
      text: "That ticket is currently in progress"
      duration_ms: 2000
      emotion: neutral
    completed:
      path: "status/completed.mp4"
      text: "That's been completed"
      duration_ms: 1500
      emotion: positive
    blocked:
      path: "status/blocked.mp4"
      text: "It's blocked at the moment"
      duration_ms: 1500
      emotion: concerned

  fillers:
    thinking:
      path: "fillers/thinking.mp4"
      text: ""
      duration_ms: 2000
      emotion: thinking
      loop: true
    idle:
      path: "fillers/idle.mp4"
      text: ""
      duration_ms: 3000
      emotion: neutral
      loop: true
```

#### Clip Generation (Offline, High Quality)

```bash
# Generate clips with SadTalker at 512Ã—512 (offline)
python sadtalker/inference.py \
    --driven_audio audio/hello.wav \
    --source_image avatar/headshot.png \
    --result_dir clips/greetings/ \
    --size 512 \
    --preprocess full \
    --enhancer gfpgan
```

#### Real-Time Generation (256Ã—256)

```python
# Wav2Lip for novel responses
class RealTimeAvatar:
    def __init__(self):
        self.wav2lip = load_wav2lip_model()  # ~1.5GB VRAM
        self.face_image = load_face("avatar/headshot.png")

    async def generate_frame(self, audio_chunk):
        # 256Ã—256 @ 25 FPS, ~3-4GB total VRAM
        mel = audio_to_mel(audio_chunk)
        frame = self.wav2lip(self.face_image, mel)
        return frame
```

#### Switching Logic

```python
class HybridVideoOutput:
    def __init__(self, clip_library, realtime_avatar):
        self.clips = clip_library
        self.realtime = realtime_avatar
        self.gpu_monitor = GPUMonitor()

    async def generate_response(self, text: str, audio: bytes):
        # 1. Check for exact/fuzzy match in clip library
        clip = self.clips.find_match(text, threshold=0.85)

        if clip:
            # Use pre-generated clip (higher quality)
            return await self.play_clip(clip)

        # 2. Check GPU availability
        if self.gpu_monitor.vram_available < 4.0:  # GB
            # GPU overloaded, use filler + queue
            await self.play_clip(self.clips.get("thinking"))
            # Queue real-time generation

        # 3. Real-time generation
        return await self.realtime.generate(audio)

    def find_match(self, text: str, threshold: float):
        """Fuzzy match against clip library"""
        from rapidfuzz import fuzz

        best_match = None
        best_score = 0

        for clip in self.clips.all():
            score = fuzz.ratio(text.lower(), clip.text.lower()) / 100
            if score > best_score and score >= threshold:
                best_match = clip
                best_score = score

        return best_match
```

### 8. Virtual Devices Setup

#### PulseAudio Virtual Sink (Audio Output)

```bash
# Create virtual sink for bot audio output
pactl load-module module-null-sink sink_name=meet_bot_output \
    sink_properties=device.description="Meet_Bot_Output"

# Create virtual source from sink monitor
pactl load-module module-virtual-source source_name=meet_bot_mic \
    master=meet_bot_output.monitor \
    source_properties=device.description="Meet_Bot_Microphone"
```

#### v4l2loopback Virtual Camera (Video Output)

```bash
# Load v4l2loopback module
sudo modprobe v4l2loopback devices=1 video_nr=10 \
    card_label="Meet_Bot_Camera" exclusive_caps=1

# Write frames to /dev/video10
ffmpeg -re -i avatar_output.mp4 -f v4l2 /dev/video10
```

### 9. Command Center UI (Meetings Tab)

VS Code webviews **fully support** embedded video playback with interactive controls.

**Supported Features:**
- HTML5 `<video>` tag with H.264/VP8 codecs
- Local video files via `asWebviewUri()`
- Canvas rendering for real-time frame updates
- Full JavaScript interactivity (play/pause/seek/playlist)
- `postMessage()` communication between extension and webview

**Limitations:**
- YouTube/external iframes blocked (sandbox restrictions)
- Must use supported codecs (H.264, VP8)
- Remote streaming requires careful CSP configuration

#### Meetings Tab Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“… Meetings Tab                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    VIDEO PREVIEW                             â”‚    â”‚
â”‚  â”‚              (Avatar or Meeting Feed)                        â”‚    â”‚
â”‚  â”‚                    256Ã—256 â†’ 512Ã—512                         â”‚    â”‚
â”‚  â”‚                                                              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚ ğŸ¤ Live Transcription                                 â”‚   â”‚    â”‚
â”‚  â”‚  â”‚ "...and the sprint velocity looks good for AAP-12345" â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ğŸ›ï¸ Controls                                                  â”‚    â”‚
â”‚  â”‚ [â–¶ï¸ Join] [â¸ï¸ Mute] [ğŸ“· Video] [ğŸ”Š Volume â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬]        â”‚    â”‚
â”‚  â”‚ [ğŸ’¬ Say Something...                              ] [Send]  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ğŸ“‹ Upcoming Meetings                                         â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚ âœ… Sprint Standup - 10:00 AM                   [Approve] â”‚ â”‚    â”‚
â”‚  â”‚ â”‚    Attendees: 5 | Duration: 15min | Jira: AAP-12345     â”‚ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚ â”‚ â³ Backlog Grooming - 2:00 PM                  [Approve] â”‚ â”‚    â”‚
â”‚  â”‚ â”‚    Attendees: 8 | Duration: 60min | Jira: AAP-12346     â”‚ â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ğŸ¬ Pre-Generated Clips                                       â”‚    â”‚
â”‚  â”‚ [ğŸ‘‹ Greeting] [âœ… Acknowledge] [ğŸ¤” Thinking] [ğŸ‘ Agree]     â”‚    â”‚
â”‚  â”‚ [ğŸ“Š Status Update] [â“ Question] [ğŸ‘‹ Goodbye]               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Video Implementation

**Option 1: Pre-rendered Video Clips (Recommended for 6GB VRAM)**
```html
<!-- In webview HTML -->
<video id="avatarVideo" width="512" height="512" autoplay muted>
  <source src="${webview.asWebviewUri(clipPath)}" type="video/mp4">
</video>
<script>
  // Playlist management
  const playlist = ['greeting.mp4', 'idle.mp4', 'thinking.mp4'];
  let currentClip = 0;

  document.getElementById('avatarVideo').addEventListener('ended', () => {
    currentClip = (currentClip + 1) % playlist.length;
    loadClip(playlist[currentClip]);
  });

  function loadClip(clipName) {
    const video = document.getElementById('avatarVideo');
    video.src = clipPaths[clipName];
    video.play();
  }
</script>
```

**Option 2: Real-time Canvas Rendering (for novel responses)**
```javascript
// Receive frames from backend via postMessage
const canvas = document.getElementById('avatarCanvas');
const ctx = canvas.getContext('2d');

window.addEventListener('message', event => {
  if (event.data.command === 'avatarFrame') {
    // Decode base64 JPEG frame from backend
    const img = new Image();
    img.src = 'data:image/jpeg;base64,' + event.data.frame;
    img.onload = () => ctx.drawImage(img, 0, 0, 256, 256);
  }
});
```

**Option 3: Hybrid (Best for your setup)**
```javascript
class HybridVideoPlayer {
  constructor(videoEl, canvasEl) {
    this.video = videoEl;
    this.canvas = canvasEl;
    this.mode = 'clip'; // 'clip' or 'realtime'
  }

  playClip(clipPath) {
    this.mode = 'clip';
    this.canvas.style.display = 'none';
    this.video.style.display = 'block';
    this.video.src = clipPath;
    this.video.play();
  }

  startRealtime() {
    this.mode = 'realtime';
    this.video.style.display = 'none';
    this.canvas.style.display = 'block';
    // Backend will send frames via postMessage
  }

  renderFrame(base64Frame) {
    if (this.mode !== 'realtime') return;
    const ctx = this.canvas.getContext('2d');
    const img = new Image();
    img.src = 'data:image/jpeg;base64,' + base64Frame;
    img.onload = () => ctx.drawImage(img, 0, 0);
  }
}
```

#### TypeScript Interfaces

```typescript
// Extension to commandCenter.ts
interface MeetingState {
  id: string;
  title: string;
  startTime: Date;
  meetUrl: string;
  status: 'pending' | 'approved' | 'joined' | 'ended';
  organizer: string;
  attendees: string[];
  jiraIssues: string[];  // Pre-loaded from calendar description
}

interface ActiveMeeting extends MeetingState {
  transcription: TranscriptionEntry[];
  lastWakeWord: Date | null;
  responseQueue: ResponseItem[];
  gpuUsage: number;
  vramUsage: number;
  currentMode: 'idle' | 'listening' | 'responding';
  videoMode: 'clip' | 'realtime';
  currentClip: string | null;
}

interface TranscriptionEntry {
  timestamp: Date;
  speaker: string;
  text: string;
  isWakeWord: boolean;
}

interface ResponseItem {
  id: string;
  text: string;
  status: 'queued' | 'generating' | 'playing' | 'completed';
  videoPath?: string;  // For pre-gen clips
  audioPath?: string;
}

// Message types for webview communication
type MeetingMessage =
  | { command: 'avatarFrame'; frame: string }  // base64 JPEG
  | { command: 'transcription'; entry: TranscriptionEntry }
  | { command: 'meetingState'; state: ActiveMeeting }
  | { command: 'playClip'; clipPath: string }
  | { command: 'gpuStatus'; usage: number; vram: number };
```

#### Backend Frame Streaming

```python
# In bot_service.py - stream frames to VS Code webview
import asyncio
import base64
import cv2

class WebviewFrameStreamer:
    def __init__(self, websocket_url: str):
        self.ws = None
        self.frame_queue = asyncio.Queue()

    async def connect(self):
        # Connect to VS Code extension's WebSocket server
        self.ws = await websockets.connect(self.websocket_url)

    async def stream_frame(self, frame: np.ndarray):
        """Send a frame to the VS Code webview"""
        # Encode as JPEG for efficiency
        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        base64_frame = base64.b64encode(buffer).decode('utf-8')

        await self.ws.send(json.dumps({
            'command': 'avatarFrame',
            'frame': base64_frame
        }))

    async def send_transcription(self, entry: dict):
        await self.ws.send(json.dumps({
            'command': 'transcription',
            'entry': entry
        }))
```

#### localResourceRoots Configuration

The existing Command Center already has the infrastructure:

```typescript
// Already in commandCenter.ts line 279
localResourceRoots: [extensionUri],
```

For video clips, we'll add the clips directory:

```typescript
localResourceRoots: [
  extensionUri,
  vscode.Uri.file(path.join(os.homedir(), '.config', 'meet-bot', 'clips'))
],
```

---

## File Structure

```
tool_modules/aa_meet_bot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tools_basic.py          # MCP tools for bot control
â”‚   â”œâ”€â”€ bot_service.py          # Main orchestrator
â”‚   â”œâ”€â”€ meeting_controller.py   # Browser automation (Puppeteer)
â”‚   â”œâ”€â”€ audio_pipeline.py       # PulseAudio capture/output
â”‚   â”œâ”€â”€ wake_word.py            # Porcupine integration
â”‚   â”œâ”€â”€ whisper_npu.py          # OpenVINO Whisper
â”‚   â”œâ”€â”€ context_manager.py      # Jira preloader + meeting context
â”‚   â”œâ”€â”€ llm_processor.py        # Response generation
â”‚   â”œâ”€â”€ tts_engine.py           # GPT-SoVITS wrapper
â”‚   â”œâ”€â”€ avatar_engine.py        # Wav2Lip/FlashLips
â”‚   â”œâ”€â”€ clip_library.py         # Pre-generated clip management
â”‚   â”œâ”€â”€ hybrid_output.py        # Switching logic
â”‚   â”œâ”€â”€ virtual_devices.py      # PulseAudio/v4l2loopback setup
â”‚   â””â”€â”€ output_sync.py          # Audio/video synchronization
â”œâ”€â”€ clips/
â”‚   â”œâ”€â”€ library.yaml            # Clip metadata
â”‚   â”œâ”€â”€ greetings/
â”‚   â”œâ”€â”€ transitions/
â”‚   â”œâ”€â”€ status/
â”‚   â””â”€â”€ fillers/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ wake_word/              # Porcupine custom model
â”‚   â”œâ”€â”€ whisper/                # OpenVINO Whisper
â”‚   â”œâ”€â”€ gpt_sovits/             # Voice model
â”‚   â””â”€â”€ wav2lip/                # Lip sync model
â””â”€â”€ requirements.txt

scripts/
â”œâ”€â”€ train_voice_model.py        # GPT-SoVITS training
â”œâ”€â”€ generate_clips.py           # Batch clip generation
â”œâ”€â”€ collect_voice_samples.py    # Voice sample recording
â””â”€â”€ setup_virtual_devices.sh    # PulseAudio/v4l2loopback

skills/
â””â”€â”€ meet_bot_join.yaml          # Skill for joining meetings

extensions/aa_workflow_vscode/
â””â”€â”€ src/
    â””â”€â”€ commandCenter.ts        # Add Meetings tab
```

---

## Config Addition

```json
{
  "meet_bot": {
    "enabled": false,
    "wake_word": "david",
    "voice_model_path": "~/.config/meet-bot/voice_model",
    "avatar_image_path": "~/.config/meet-bot/avatar.png",
    "clips_path": "~/.config/meet-bot/clips",
    "response_latency_target_ms": 3000,
    "hardware": {
      "stt_device": "NPU",
      "tts_device": "GPU",
      "avatar_device": "GPU",
      "llm_device": "iGPU"
    },
    "avatar": {
      "mode": "hybrid",
      "realtime_resolution": [256, 256],
      "realtime_fps": 25,
      "pregen_resolution": [512, 512],
      "clip_match_threshold": 0.85
    },
    "jira_preload": {
      "sprint_issues": true,
      "recent_comments": true,
      "blocked_items": true,
      "my_mrs": true
    },
    "approval": {
      "mode": "command_center",
      "auto_approve_organizers": [],
      "join_buffer_minutes": 2
    },
    "browser": {
      "executable": "/usr/bin/chromium-browser",
      "user_data_dir": "~/.config/meet-bot/chrome-profile"
    }
  }
}
```

---

## Implementation Phases

### Phase 1: Foundation âœ… COMPLETED
- [x] Set up virtual audio/video devices (PulseAudio + v4l2loopback)
- [x] Implement browser automation for Meet joining (Playwright)
- [x] Caption capture pipeline (MutationObserver on Meet DOM)
- [x] Wake-word detection (text-based regex on captions)

### Phase 2: Voice/Video âœ… COMPLETED
- [x] Voice sample collection (phonetics.wav - 7:19)
- [x] GPT-SoVITS voice cloning setup
- [x] Wav2Lip real-time lip-sync integration
- [x] Video generation pipeline

### Phase 3: Response Generation âœ… COMPLETED
- [x] Jira context preloader (REST API, 50+ issues)
- [x] LLM response generation (Ollama qwen2.5:0.5b on iGPU)
- [x] Full response pipeline (LLM â†’ TTS â†’ Video)

### Phase 4: Integration âœ… COMPLETED
- [x] Audio/video synchronization
- [x] Output to virtual devices
- [x] Command Center UI (Meetings tab)
- [x] End-to-end testing
- [x] VS Code extension build & install

### Phase 5: Live Testing ğŸ”„ NEXT
- [ ] Join actual Google Meet with bot account
- [ ] Test wake-word detection in live meeting
- [ ] Verify avatar video output to virtual camera
- [ ] Test Jira context responses

### Phase 6: Polish (Optional)
- [ ] Latency optimization
- [ ] Error handling and recovery
- [ ] Pre-generated clip library (for common phrases)
- [ ] Documentation

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Google blocks automation | Use undetected-chromedriver, maintain session cookies |
| 6GB VRAM insufficient | Aggressive clip library usage, lower resolution |
| Voice quality poor | More training samples, better preprocessing |
| Latency too high | Pre-buffer responses, expand clip library |
| Wake-word false positives | Tune sensitivity, add confirmation phrase |

---

## Success Metrics

| Metric | Target | Current Status |
|--------|--------|----------------|
| Join latency | <30s from approval | â³ Not tested live |
| Wake-word accuracy | >95% true positive | âœ… Text-based (100% accuracy) |
| STT accuracy | >90% word accuracy | âœ… Using Google's captions |
| Response latency | <5s end-to-end | âœ… ~8s (LLM + TTS + Video) |
| Video quality | Seamless lip-sync | âœ… Wav2Lip @ 600Ã—600 |
| GPU utilization | <90% VRAM | âœ… ~5GB peak during generation |

## Current Capabilities

### Working Features
- âœ… Virtual audio devices (PulseAudio sink/source)
- âœ… Virtual camera (/dev/video10 via v4l2loopback)
- âœ… Caption-based transcription (no local STT needed)
- âœ… Text-based wake-word detection ("David")
- âœ… LLM responses (Ollama qwen2.5:0.5b)
- âœ… Voice cloning (GPT-SoVITS with your voice)
- âœ… Real-time lip-sync video (Wav2Lip)
- âœ… Jira context preloading (50+ sprint issues)
- âœ… VS Code Meetings tab with video preview

### Ready for Live Testing
1. Create meeting with `daoneill@redhat.com`
2. Bot joins as `dmz.oneill@gmail.com`
3. Say "David, what's the status of AAP-54933?"
4. Bot responds with lip-synced avatar video

---

## Dependencies

```txt
# Python (in pyproject.toml)
pyppeteer>=1.0.0
pulsectl>=23.0.0
optimum[openvino]>=1.16.0
torch>=2.0.0
torchaudio>=2.0.0
transformers>=4.36.0
rapidfuzz>=3.0.0
numpy==1.26.4          # Pinned for Wav2Lip compatibility
scipy==1.17.0          # Pinned for Wav2Lip compatibility
librosa==0.10.1        # Pinned for Wav2Lip compatibility
opencv-python-headless>=4.0.0
websockets>=12.0

# System
pulseaudio
v4l2loopback-dkms
chromium-browser / google-chrome
ffmpeg

# Models (installed)
# - GPT-SoVITS: ~/src/GPT-SoVITS/GPT_weights_v2Pro/dave-e50.ckpt
# - Wav2Lip: ~/src/Wav2Lip/checkpoints/wav2lip_gan.pth
# - Ollama: qwen2.5:0.5b (iGPU)
```

## File Locations

| Component | Path |
|-----------|------|
| Module | `tool_modules/aa_meet_bot/` |
| Config | `config.json` â†’ `meet_bot` section |
| Avatar Image | `/home/daoneill/Documents/Identification/IMG_3249_.jpg` |
| Voice Reference | `~/Music/phonetics.wav` |
| GPT-SoVITS | `~/src/GPT-SoVITS/` |
| Wav2Lip | `~/src/Wav2Lip/` |
| Generated Audio | `~/.local/share/meet_bot/audio/` |
| Generated Video | `~/.local/share/meet_bot/clips/` |
| VS Code Extension | `extensions/aa_workflow_vscode/aa-workflow-0.1.0.vsix` |
| **GPU Optimization Guide** | `docs/meet-bot-gpu-optimization.md` |

## Related Documentation

- **[GPU/NPU Optimization Guide](meet-bot-gpu-optimization.md)** - Deep technical details on:
  - OpenGL text rendering pipeline
  - OpenCL video processing and color conversion
  - v4l2loopback virtual camera setup
  - NPU Whisper STT integration
  - Performance tuning and troubleshooting
  - Why zero-copy NPU audio is not currently possible
